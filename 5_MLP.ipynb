{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeminq1/python_ai_colab/blob/main/5_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras: a library for deep learning (the easiest)\n",
        "\n",
        "\n",
        "https://keras.io/api/"
      ],
      "metadata": {
        "id": "uXAre6q6y3VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "qF6zjmamzAtz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwg3JWBQZuNt"
      },
      "source": [
        "### MLP (multi-layer perceptron, fully-connected)\n",
        "#### - regression, classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRXhGVcBcOsz"
      },
      "source": [
        "### 1. Regression\n",
        "#### - use the last score as prediction score\n",
        "#### - Boston housing price dataset (regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvUUxKErcP1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b2dc40-9772-4752-d8c1-99ed5a40939e"
      },
      "source": [
        "## Dataset load (https://keras.io/api/datasets/boston_housing/)\n",
        "(X_train, y_train), (X_test, y_test) =keras.datasets.boston_housing.load_data(\n",
        "    path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
        ")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## data shape\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "QjOz-HeBKRiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c880fe6-4c79-45e3-e4f6-7846f709391e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13)\n",
            "(102, 13)\n",
            "(404,)\n",
            "(102,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Data sample\n",
        "y_train[0]\n"
      ],
      "metadata": {
        "id": "tdc57Ya5P4LD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71bbd990-1fef-4f04-81c1-373e9babeffb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tixdf84Cc8Oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1ad0cf-e0a6-42cc-8a7c-ae061a775640"
      },
      "source": [
        "## Model (https://keras.io/guides/sequential_model/)\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(6, activation=\"relu\", name=\"layer1\",input_dim=13),\n",
        "        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
        "        layers.Dense(1,name=\"layer3\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer1 (Dense)              (None, 6)                 84        \n",
            "                                                                 \n",
            " layer2 (Dense)              (None, 3)                 21        \n",
            "                                                                 \n",
            " layer3 (Dense)              (None, 1)                 4         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109\n",
            "Trainable params: 109\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zngpBtGMdnB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593e7f72-7182-4d59-d62e-9474bd1dc742"
      },
      "source": [
        "## Train (https://keras.io/api/models/model_training_apis/)\n",
        "batch_size = 16 \n",
        "epochs = 200\n",
        "\n",
        "## compile\n",
        "# 러닝 rate / loss / optimaizer 머 쓸건지 정함\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=keras.losses.MeanSquaredError())\n",
        "\n",
        "## fit\n",
        "model.fit(x=X_train,y=y_train,batch_size=batch_size,epochs=epochs,validation_split=0.1)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "23/23 [==============================] - 3s 11ms/step - loss: 589.8611 - val_loss: 490.6507\n",
            "Epoch 2/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 517.2059 - val_loss: 444.1240\n",
            "Epoch 3/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 423.1861 - val_loss: 233.4456\n",
            "Epoch 4/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 266.3398 - val_loss: 111.2544\n",
            "Epoch 5/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 182.0485 - val_loss: 69.6169\n",
            "Epoch 6/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 139.6757 - val_loss: 60.8821\n",
            "Epoch 7/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 121.5915 - val_loss: 57.0223\n",
            "Epoch 8/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 111.9341 - val_loss: 56.8944\n",
            "Epoch 9/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 105.1487 - val_loss: 49.1824\n",
            "Epoch 10/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 98.3034 - val_loss: 49.5541\n",
            "Epoch 11/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 94.5660 - val_loss: 50.0843\n",
            "Epoch 12/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 92.3115 - val_loss: 44.0442\n",
            "Epoch 13/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 90.1697 - val_loss: 46.0076\n",
            "Epoch 14/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 86.8586 - val_loss: 45.4554\n",
            "Epoch 15/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.1436 - val_loss: 47.0610\n",
            "Epoch 16/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 84.4320 - val_loss: 44.2864\n",
            "Epoch 17/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 83.0626 - val_loss: 41.4227\n",
            "Epoch 18/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 83.4465 - val_loss: 42.0227\n",
            "Epoch 19/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 81.0170 - val_loss: 47.2808\n",
            "Epoch 20/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 80.9211 - val_loss: 40.4596\n",
            "Epoch 21/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 78.8889 - val_loss: 43.7612\n",
            "Epoch 22/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 78.2538 - val_loss: 42.2665\n",
            "Epoch 23/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 78.6338 - val_loss: 40.6832\n",
            "Epoch 24/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 77.3054 - val_loss: 49.6857\n",
            "Epoch 25/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 76.5012 - val_loss: 38.2485\n",
            "Epoch 26/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 76.1729 - val_loss: 38.3711\n",
            "Epoch 27/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 73.2908 - val_loss: 40.2758\n",
            "Epoch 28/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 73.0343 - val_loss: 37.1544\n",
            "Epoch 29/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 72.4420 - val_loss: 39.3630\n",
            "Epoch 30/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 71.7636 - val_loss: 38.3426\n",
            "Epoch 31/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 71.0434 - val_loss: 39.1449\n",
            "Epoch 32/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.8694 - val_loss: 41.6344\n",
            "Epoch 33/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.5111 - val_loss: 36.7014\n",
            "Epoch 34/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.8790 - val_loss: 36.0656\n",
            "Epoch 35/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.3670 - val_loss: 36.2851\n",
            "Epoch 36/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 66.5494 - val_loss: 35.0315\n",
            "Epoch 37/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 67.5722 - val_loss: 35.3949\n",
            "Epoch 38/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.4391 - val_loss: 35.6164\n",
            "Epoch 39/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 69.4583 - val_loss: 37.6626\n",
            "Epoch 40/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 64.2956 - val_loss: 34.4932\n",
            "Epoch 41/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 66.3862 - val_loss: 36.5061\n",
            "Epoch 42/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 64.9836 - val_loss: 34.2481\n",
            "Epoch 43/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 62.7941 - val_loss: 37.7317\n",
            "Epoch 44/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 62.6451 - val_loss: 37.1780\n",
            "Epoch 45/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 61.5897 - val_loss: 35.9100\n",
            "Epoch 46/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 60.8836 - val_loss: 34.3935\n",
            "Epoch 47/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 60.2585 - val_loss: 35.6831\n",
            "Epoch 48/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 60.9767 - val_loss: 37.3059\n",
            "Epoch 49/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 59.2065 - val_loss: 34.1505\n",
            "Epoch 50/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 60.1852 - val_loss: 34.4702\n",
            "Epoch 51/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.6306 - val_loss: 34.4230\n",
            "Epoch 52/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 56.5647 - val_loss: 37.4821\n",
            "Epoch 53/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.8937 - val_loss: 34.7878\n",
            "Epoch 54/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 57.2353 - val_loss: 34.7193\n",
            "Epoch 55/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 54.9841 - val_loss: 36.3888\n",
            "Epoch 56/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 56.1952 - val_loss: 35.2701\n",
            "Epoch 57/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 55.3309 - val_loss: 36.6868\n",
            "Epoch 58/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 53.6896 - val_loss: 35.1345\n",
            "Epoch 59/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 54.5388 - val_loss: 36.2223\n",
            "Epoch 60/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 53.0721 - val_loss: 35.3020\n",
            "Epoch 61/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.4244 - val_loss: 35.4418\n",
            "Epoch 62/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.4491 - val_loss: 36.2421\n",
            "Epoch 63/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.5929 - val_loss: 36.3077\n",
            "Epoch 64/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.4791 - val_loss: 40.9066\n",
            "Epoch 65/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 52.4903 - val_loss: 35.9674\n",
            "Epoch 66/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.6737 - val_loss: 36.1609\n",
            "Epoch 67/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.1864 - val_loss: 36.4652\n",
            "Epoch 68/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 48.4958 - val_loss: 36.2893\n",
            "Epoch 69/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 48.5675 - val_loss: 36.7483\n",
            "Epoch 70/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 48.3144 - val_loss: 36.5563\n",
            "Epoch 71/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.8605 - val_loss: 37.2197\n",
            "Epoch 72/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.2220 - val_loss: 38.6201\n",
            "Epoch 73/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 46.5668 - val_loss: 37.3776\n",
            "Epoch 74/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 47.4251 - val_loss: 36.8323\n",
            "Epoch 75/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.7416 - val_loss: 32.7891\n",
            "Epoch 76/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 43.0347 - val_loss: 35.1209\n",
            "Epoch 77/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 42.4244 - val_loss: 31.8352\n",
            "Epoch 78/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.3889 - val_loss: 29.9221\n",
            "Epoch 79/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.1426 - val_loss: 29.6159\n",
            "Epoch 80/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.7376 - val_loss: 30.3130\n",
            "Epoch 81/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.6429 - val_loss: 31.5184\n",
            "Epoch 82/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.4814 - val_loss: 31.8053\n",
            "Epoch 83/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.3116 - val_loss: 27.9531\n",
            "Epoch 84/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.6831 - val_loss: 27.8947\n",
            "Epoch 85/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.0719 - val_loss: 32.6730\n",
            "Epoch 86/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.6505 - val_loss: 31.8493\n",
            "Epoch 87/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.1601 - val_loss: 31.0224\n",
            "Epoch 88/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.8116 - val_loss: 27.6625\n",
            "Epoch 89/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.1953 - val_loss: 27.4751\n",
            "Epoch 90/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.8314 - val_loss: 27.6824\n",
            "Epoch 91/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.1758 - val_loss: 28.1785\n",
            "Epoch 92/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.8835 - val_loss: 28.1304\n",
            "Epoch 93/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.9315 - val_loss: 30.2359\n",
            "Epoch 94/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.3837 - val_loss: 29.8645\n",
            "Epoch 95/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.7608 - val_loss: 30.6714\n",
            "Epoch 96/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.5469 - val_loss: 30.5952\n",
            "Epoch 97/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.8553 - val_loss: 30.3075\n",
            "Epoch 98/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.7780 - val_loss: 28.9152\n",
            "Epoch 99/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.9758 - val_loss: 30.2210\n",
            "Epoch 100/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.4412 - val_loss: 31.2618\n",
            "Epoch 101/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.3911 - val_loss: 29.8066\n",
            "Epoch 102/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.4286 - val_loss: 29.5496\n",
            "Epoch 103/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.9236 - val_loss: 30.4918\n",
            "Epoch 104/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.9880 - val_loss: 30.5086\n",
            "Epoch 105/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.5754 - val_loss: 32.5736\n",
            "Epoch 106/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.5363 - val_loss: 29.7474\n",
            "Epoch 107/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.3878 - val_loss: 30.5639\n",
            "Epoch 108/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.0331 - val_loss: 30.0309\n",
            "Epoch 109/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.3951 - val_loss: 31.1957\n",
            "Epoch 110/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.2741 - val_loss: 29.9462\n",
            "Epoch 111/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.8185 - val_loss: 31.2455\n",
            "Epoch 112/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.8448 - val_loss: 30.4661\n",
            "Epoch 113/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.5908 - val_loss: 29.6969\n",
            "Epoch 114/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.5143 - val_loss: 31.8375\n",
            "Epoch 115/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.8551 - val_loss: 33.8074\n",
            "Epoch 116/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.2820 - val_loss: 30.3422\n",
            "Epoch 117/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.6653 - val_loss: 29.7610\n",
            "Epoch 118/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.9734 - val_loss: 29.9212\n",
            "Epoch 119/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.6334 - val_loss: 33.2414\n",
            "Epoch 120/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.7318 - val_loss: 33.4327\n",
            "Epoch 121/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.7083 - val_loss: 31.1870\n",
            "Epoch 122/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.5796 - val_loss: 30.8189\n",
            "Epoch 123/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.4567 - val_loss: 30.6373\n",
            "Epoch 124/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.6593 - val_loss: 30.8793\n",
            "Epoch 125/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.9845 - val_loss: 30.5085\n",
            "Epoch 126/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.3961 - val_loss: 30.2165\n",
            "Epoch 127/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.6323 - val_loss: 30.8999\n",
            "Epoch 128/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.5101 - val_loss: 29.8265\n",
            "Epoch 129/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.3679 - val_loss: 29.8760\n",
            "Epoch 130/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.0149 - val_loss: 30.0901\n",
            "Epoch 131/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.6597 - val_loss: 30.2838\n",
            "Epoch 132/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.3254 - val_loss: 29.9843\n",
            "Epoch 133/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.5517 - val_loss: 31.0854\n",
            "Epoch 134/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.1399 - val_loss: 29.9738\n",
            "Epoch 135/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.2003 - val_loss: 31.5091\n",
            "Epoch 136/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.9191 - val_loss: 30.9153\n",
            "Epoch 137/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.6995 - val_loss: 30.7772\n",
            "Epoch 138/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.0341 - val_loss: 30.5368\n",
            "Epoch 139/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.1808 - val_loss: 30.0577\n",
            "Epoch 140/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.4368 - val_loss: 29.9872\n",
            "Epoch 141/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.6174 - val_loss: 29.9135\n",
            "Epoch 142/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.6139 - val_loss: 30.0119\n",
            "Epoch 143/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.8554 - val_loss: 30.3783\n",
            "Epoch 144/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.3318 - val_loss: 31.7148\n",
            "Epoch 145/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.7743 - val_loss: 30.1935\n",
            "Epoch 146/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.7718 - val_loss: 30.2386\n",
            "Epoch 147/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.3586 - val_loss: 29.8687\n",
            "Epoch 148/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.6234 - val_loss: 29.8040\n",
            "Epoch 149/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.0434 - val_loss: 30.2078\n",
            "Epoch 150/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.5903 - val_loss: 29.8960\n",
            "Epoch 151/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.5473 - val_loss: 29.7699\n",
            "Epoch 152/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4641 - val_loss: 30.0432\n",
            "Epoch 153/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.8349 - val_loss: 30.1861\n",
            "Epoch 154/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.3687 - val_loss: 30.0151\n",
            "Epoch 155/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4727 - val_loss: 30.4022\n",
            "Epoch 156/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.6884 - val_loss: 31.2108\n",
            "Epoch 157/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.1746 - val_loss: 31.2471\n",
            "Epoch 158/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.7634 - val_loss: 31.0024\n",
            "Epoch 159/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.3229 - val_loss: 31.7242\n",
            "Epoch 160/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.8041 - val_loss: 30.2981\n",
            "Epoch 161/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.9335 - val_loss: 30.1130\n",
            "Epoch 162/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.5652 - val_loss: 29.9198\n",
            "Epoch 163/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.3516 - val_loss: 31.9147\n",
            "Epoch 164/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.6552 - val_loss: 29.6467\n",
            "Epoch 165/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.7356 - val_loss: 29.7643\n",
            "Epoch 166/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.3837 - val_loss: 30.0952\n",
            "Epoch 167/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.9465 - val_loss: 31.2419\n",
            "Epoch 168/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.8357 - val_loss: 29.8709\n",
            "Epoch 169/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.7307 - val_loss: 29.7598\n",
            "Epoch 170/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.9245 - val_loss: 32.3333\n",
            "Epoch 171/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.1519 - val_loss: 30.0582\n",
            "Epoch 172/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4352 - val_loss: 30.2841\n",
            "Epoch 173/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.7142 - val_loss: 29.9157\n",
            "Epoch 174/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.3115 - val_loss: 30.3281\n",
            "Epoch 175/200\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 27.7270 - val_loss: 29.6852\n",
            "Epoch 176/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.6720 - val_loss: 29.9498\n",
            "Epoch 177/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.1277 - val_loss: 29.8339\n",
            "Epoch 178/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.8198 - val_loss: 29.5810\n",
            "Epoch 179/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.5462 - val_loss: 30.2018\n",
            "Epoch 180/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.8876 - val_loss: 29.6466\n",
            "Epoch 181/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.8914 - val_loss: 29.5637\n",
            "Epoch 182/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.5202 - val_loss: 29.6692\n",
            "Epoch 183/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.1605 - val_loss: 29.6986\n",
            "Epoch 184/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.6634 - val_loss: 29.8556\n",
            "Epoch 185/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.9895 - val_loss: 30.4858\n",
            "Epoch 186/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.2312 - val_loss: 30.7958\n",
            "Epoch 187/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.2263 - val_loss: 30.6390\n",
            "Epoch 188/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.1633 - val_loss: 29.8422\n",
            "Epoch 189/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.7116 - val_loss: 30.8390\n",
            "Epoch 190/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.5802 - val_loss: 30.2889\n",
            "Epoch 191/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.2862 - val_loss: 29.7168\n",
            "Epoch 192/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4072 - val_loss: 33.0137\n",
            "Epoch 193/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.3756 - val_loss: 29.8762\n",
            "Epoch 194/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.1350 - val_loss: 31.2099\n",
            "Epoch 195/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.5321 - val_loss: 29.6519\n",
            "Epoch 196/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4439 - val_loss: 31.2165\n",
            "Epoch 197/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.8878 - val_loss: 29.8083\n",
            "Epoch 198/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.3505 - val_loss: 34.2452\n",
            "Epoch 199/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.5902 - val_loss: 29.5426\n",
            "Epoch 200/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.7176 - val_loss: 30.2724\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6df0431dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKLsdeCMiHut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797eef36-edd7-4001-f54c-9130a9380067"
      },
      "source": [
        "## evaluate on the test set.(https://keras.io/api/models/model_training_apis/)\n",
        "model.evaluate(\n",
        "    x=X_test,\n",
        "    y=y_test,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 27.1917\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27.191665649414062"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## prediction on new sample (https://keras.io/api/models/model_training_apis/)\n",
        "\n",
        "model.predict(X_test[0].reshape(-1,13))"
      ],
      "metadata": {
        "id": "7X0VUuz40K27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3768411d-dea0-4ead-cf35-458715e1dfcb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.581277]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAbWjsZX9GsP"
      },
      "source": [
        "### 2. Classification\n",
        "#### - use softmax layer at the end of network to produce \"probability\".\n",
        "\n",
        "#### - MNIST dataset\n",
        "#### - images of numbers (0~9) -> classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zK2d0oG9c0s"
      },
      "source": [
        "## Download MNIST dataset (https://keras.io/api/datasets/mnist/)\n",
        "(X_train, y_train), (X_test, y_test) =keras.datasets.mnist.load_data(path=\"mnist.npz\")\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogbmtvqHplKN",
        "outputId": "add32332-9b5c-4399-a49e-f611f82aafa8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the shape of data from (W, W) to (W*W, )\n",
        "X_train = X_train.reshape((-1, 28*28))\n",
        "X_test = X_test.reshape((-1, 28*28))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "W5e_yaKlxNR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1952baf-034a-4ace-f5fb-b99a5bca9e82"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(60000,)\n",
            "(10000, 784)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# plt.figure()\n",
        "# plt.imshow(X_train[0])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "L7y8gp6zk3Pm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert class vectors to binary class matrices (google it!: https://www.educative.io/edpresso/how-to-perform-one-hot-encoding-using-keras)\n",
        "# output shape 맞춰줌\n",
        "# 현재는 output에 숫자 1 / 2 /3 이런식으로 들어가 있는데, output 0,1 인 ont hot vector 로 바꿔줘야한다.\n",
        "# y_train[0]=5 인 것을 [0,0,0,0,1,0,0] 이런식으로 바꿔줌\n",
        "# from keras.utils import to_categorical\n",
        "# 총 label이 10개이므로, 10으로 변경해줌\n",
        "y_train = keras.utils.to_categorical(y_train,10)\n",
        "y_test = keras.utils.to_categorical(y_test,10)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "c1EBdLAHxXf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a35683-02c6-4090-9956-c05acc95354e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 10)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTEgUbPk9o2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5414e915-b9e6-4894-91a0-612648881f53"
      },
      "source": [
        "## Model (https://keras.io/guides/sequential_model/)\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(200, activation=\"relu\", name=\"layer1\",input_dim=784),\n",
        "        layers.Dense(100, activation=\"relu\", name=\"layer2\"),\n",
        "        layers.Dense(10,activation=\"softmax\",name=\"layer3\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer1 (Dense)              (None, 200)               157000    \n",
            "                                                                 \n",
            " layer2 (Dense)              (None, 100)               20100     \n",
            "                                                                 \n",
            " layer3 (Dense)              (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 178,110\n",
            "Trainable params: 178,110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGzW1cFaD-xP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9857681a-84da-4e16-a9d9-26b68a5939e0"
      },
      "source": [
        "## Train (https://keras.io/api/models/model_training_apis/)\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "## compile\n",
        "# 러닝 rate / loss / optimaizer 머 쓸건지 정함\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) # keras.losses.MeanSq\n",
        "## fit\n",
        "hist = model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "844/844 [==============================] - 5s 3ms/step - loss: 2.3693 - accuracy: 0.8734 - val_loss: 0.4439 - val_accuracy: 0.9195\n",
            "Epoch 2/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.3343 - accuracy: 0.9318 - val_loss: 0.2799 - val_accuracy: 0.9388\n",
            "Epoch 3/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.2075 - accuracy: 0.9485 - val_loss: 0.2187 - val_accuracy: 0.9513\n",
            "Epoch 4/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.1565 - accuracy: 0.9578 - val_loss: 0.1793 - val_accuracy: 0.9573\n",
            "Epoch 5/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.1374 - accuracy: 0.9615 - val_loss: 0.1972 - val_accuracy: 0.9562\n",
            "Epoch 6/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.1246 - accuracy: 0.9649 - val_loss: 0.1808 - val_accuracy: 0.9545\n",
            "Epoch 7/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.1180 - accuracy: 0.9667 - val_loss: 0.1456 - val_accuracy: 0.9630\n",
            "Epoch 8/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.1080 - accuracy: 0.9703 - val_loss: 0.1554 - val_accuracy: 0.9640\n",
            "Epoch 9/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0966 - accuracy: 0.9722 - val_loss: 0.1617 - val_accuracy: 0.9597\n",
            "Epoch 10/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0873 - accuracy: 0.9750 - val_loss: 0.1408 - val_accuracy: 0.9687\n",
            "Epoch 11/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0830 - accuracy: 0.9766 - val_loss: 0.1506 - val_accuracy: 0.9667\n",
            "Epoch 12/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0730 - accuracy: 0.9790 - val_loss: 0.1857 - val_accuracy: 0.9652\n",
            "Epoch 13/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0747 - accuracy: 0.9786 - val_loss: 0.1535 - val_accuracy: 0.9693\n",
            "Epoch 14/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9790 - val_loss: 0.1294 - val_accuracy: 0.9735\n",
            "Epoch 15/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0610 - accuracy: 0.9825 - val_loss: 0.1567 - val_accuracy: 0.9690\n",
            "Epoch 16/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0610 - accuracy: 0.9829 - val_loss: 0.1414 - val_accuracy: 0.9707\n",
            "Epoch 17/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0550 - accuracy: 0.9847 - val_loss: 0.1260 - val_accuracy: 0.9752\n",
            "Epoch 18/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0483 - accuracy: 0.9867 - val_loss: 0.1458 - val_accuracy: 0.9705\n",
            "Epoch 19/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0582 - accuracy: 0.9849 - val_loss: 0.1738 - val_accuracy: 0.9717\n",
            "Epoch 20/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0569 - accuracy: 0.9850 - val_loss: 0.1382 - val_accuracy: 0.9735\n",
            "Epoch 21/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0498 - accuracy: 0.9874 - val_loss: 0.1993 - val_accuracy: 0.9687\n",
            "Epoch 22/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0516 - accuracy: 0.9866 - val_loss: 0.1582 - val_accuracy: 0.9715\n",
            "Epoch 23/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0441 - accuracy: 0.9881 - val_loss: 0.1530 - val_accuracy: 0.9728\n",
            "Epoch 24/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0429 - accuracy: 0.9893 - val_loss: 0.1371 - val_accuracy: 0.9757\n",
            "Epoch 25/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0402 - accuracy: 0.9894 - val_loss: 0.1187 - val_accuracy: 0.9773\n",
            "Epoch 26/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0432 - accuracy: 0.9890 - val_loss: 0.1733 - val_accuracy: 0.9717\n",
            "Epoch 27/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0381 - accuracy: 0.9898 - val_loss: 0.1763 - val_accuracy: 0.9742\n",
            "Epoch 28/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0466 - accuracy: 0.9887 - val_loss: 0.1670 - val_accuracy: 0.9740\n",
            "Epoch 29/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0392 - accuracy: 0.9904 - val_loss: 0.1701 - val_accuracy: 0.9752\n",
            "Epoch 30/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0382 - accuracy: 0.9909 - val_loss: 0.1863 - val_accuracy: 0.9748\n",
            "Epoch 31/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0352 - accuracy: 0.9915 - val_loss: 0.1728 - val_accuracy: 0.9750\n",
            "Epoch 32/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0408 - accuracy: 0.9906 - val_loss: 0.2300 - val_accuracy: 0.9683\n",
            "Epoch 33/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0402 - accuracy: 0.9914 - val_loss: 0.1824 - val_accuracy: 0.9722\n",
            "Epoch 34/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0335 - accuracy: 0.9921 - val_loss: 0.1808 - val_accuracy: 0.9758\n",
            "Epoch 35/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0405 - accuracy: 0.9917 - val_loss: 0.1844 - val_accuracy: 0.9758\n",
            "Epoch 36/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0262 - accuracy: 0.9939 - val_loss: 0.1907 - val_accuracy: 0.9740\n",
            "Epoch 37/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0355 - accuracy: 0.9922 - val_loss: 0.2078 - val_accuracy: 0.9725\n",
            "Epoch 38/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0413 - accuracy: 0.9917 - val_loss: 0.2209 - val_accuracy: 0.9725\n",
            "Epoch 39/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0289 - accuracy: 0.9936 - val_loss: 0.2003 - val_accuracy: 0.9748\n",
            "Epoch 40/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0300 - accuracy: 0.9934 - val_loss: 0.2165 - val_accuracy: 0.9737\n",
            "Epoch 41/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0329 - accuracy: 0.9928 - val_loss: 0.2208 - val_accuracy: 0.9753\n",
            "Epoch 42/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0273 - accuracy: 0.9940 - val_loss: 0.2143 - val_accuracy: 0.9732\n",
            "Epoch 43/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0357 - accuracy: 0.9929 - val_loss: 0.2580 - val_accuracy: 0.9697\n",
            "Epoch 44/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0335 - accuracy: 0.9930 - val_loss: 0.2149 - val_accuracy: 0.9757\n",
            "Epoch 45/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0317 - accuracy: 0.9929 - val_loss: 0.2833 - val_accuracy: 0.9757\n",
            "Epoch 46/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0420 - accuracy: 0.9920 - val_loss: 0.2567 - val_accuracy: 0.9773\n",
            "Epoch 47/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0315 - accuracy: 0.9935 - val_loss: 0.2802 - val_accuracy: 0.9755\n",
            "Epoch 48/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0334 - accuracy: 0.9933 - val_loss: 0.2515 - val_accuracy: 0.9752\n",
            "Epoch 49/50\n",
            "844/844 [==============================] - 2s 3ms/step - loss: 0.0336 - accuracy: 0.9939 - val_loss: 0.2791 - val_accuracy: 0.9735\n",
            "Epoch 50/50\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0250 - accuracy: 0.9943 - val_loss: 0.2669 - val_accuracy: 0.9760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TruvpcHI9Pv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "b36ecb12-8d07-410a-9869-1b39984c6c10"
      },
      "source": [
        "## plot loss and accuracy to check if the model is converged.\n",
        "# plot을 위해 hist에 저장하여 plot 한다.\n",
        "val_accuracy = hist.history['val_accuracy']\n",
        "train_accuracy = hist.history['accuracy']\n",
        "\n",
        "# x축을 epochs\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(np.arange(epochs),val_accuracy,label=\"val_accuracy\")\n",
        "plt.plot(np.arange(epochs),train_accuracy,label=\"train_accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WXq3K5IKMg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e21fa83-d86b-4a55-c09c-957c80b64ebf"
      },
      "source": [
        "## evaluate on the test set\n",
        "## you should get acc higher than 0.96\n",
        "model.evaluate(X_test,y_test,verbose=1)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2385 - accuracy: 0.9771\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2385353446006775, 0.9771000146865845]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## predict\n"
      ],
      "metadata": {
        "id": "t70X2jKqg7km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2zFMu1yXgpH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rt9igzuOhXSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Input Normalization\n",
        "#### - Make the input features normalized\n",
        "#### - why? - weight initialization\n",
        "\n",
        "\n",
        "#### - Usually adopt z-score (standard score)\n",
        "#### -> z = (x-μ)/σ"
      ],
      "metadata": {
        "id": "4d-tpXzhyxOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "jZL5a7sf2TbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Download MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) =keras.datasets.mnist.load_data(path=\"mnist.npz\")"
      ],
      "metadata": {
        "id": "ghYdTE5l2VM-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the shape of data from (W, W) to (W*W, )\n",
        "X_train = X_train.reshape((-1, 28*28))\n",
        "X_test = X_test.reshape((-1, 28*28))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "nCKcd19YGfXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e521a1-807f-4deb-aec6-223c5bea5bda"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n",
            "(60000,)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the sample mean and std\n",
        "mu = X_train.mean()\n",
        "sig = X_train.std()+0.000000001\n",
        "\n",
        "# normalize (z-score)\n",
        "X_train = (X_train - mu) / sig\n",
        "print(X_train[0])\n",
        "\n",
        "# train set과 동일한 평균과 표준편차로 test_set도 변경해줘야한다.\n",
        "\n",
        "X_test = (X_test - mu) / sig # note! : use the same statistic with the training set!"
      ],
      "metadata": {
        "id": "SbK7s4nK2ZLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f911aadb-c94f-4e91-f6b9-dd60267a41ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.38589016 -0.1949715  -0.1949715  -0.1949715\n",
            "  1.17964286  1.30692197  1.80331049 -0.09314822  1.68875929  2.82154335\n",
            "  2.71972006  1.19237077 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.04223657  0.03413089  0.77234972  1.53602436\n",
            "  1.73967093  2.79608752  2.79608752  2.79608752  2.79608752  2.79608752\n",
            "  2.43970602  1.76512675  2.79608752  2.65608051  2.0578687   0.39051239\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.19959373\n",
            "  2.60516886  2.79608752  2.79608752  2.79608752  2.79608752  2.79608752\n",
            "  2.79608752  2.79608752  2.79608752  2.7706317   0.7596218   0.61961479\n",
            "  0.61961479  0.28868911  0.07231462 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.1949715   2.36333856  2.79608752\n",
            "  2.79608752  2.79608752  2.79608752  2.79608752  2.09605243  1.89240586\n",
            "  2.71972006  2.6433526  -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389  0.59415897  1.56148018  0.93781256  2.79608752\n",
            "  2.79608752  2.18514781 -0.28406688 -0.42407389  0.12322627  1.53602436\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.24588314 -0.41134598  1.53602436  2.79608752  0.72143807\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389  1.3451057   2.79608752  1.99422915 -0.39861807 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.28406688\n",
            "  1.99422915  2.79608752  0.46687986 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.02140298  2.6433526\n",
            "  2.43970602  1.61239182  0.95054047 -0.41134598 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389  0.60688688  2.63062468  2.79608752\n",
            "  2.79608752  1.09054748 -0.10587613 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389  0.14868209  1.9433175   2.79608752  2.79608752\n",
            "  1.48511272 -0.0804203  -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.22042732  0.7596218   2.78335961  2.79608752  1.95604541\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389  2.74517588  2.79608752  2.74517588  0.39051239 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389  0.16141     1.2305545   1.90513377  2.79608752\n",
            "  2.79608752  2.21060363 -0.39861807 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.07231462  1.4596569\n",
            "  2.49061767  2.79608752  2.79608752  2.79608752  2.75790379  1.89240586\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.11860404  1.02690793  2.38879438  2.79608752  2.79608752  2.79608752\n",
            "  2.79608752  2.13423617  0.56870314 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.13133195  0.41596821  2.28697109  2.79608752\n",
            "  2.79608752  2.79608752  2.79608752  2.09605243  0.60688688 -0.39861807\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.1949715   1.75239884\n",
            "  2.36333856  2.79608752  2.79608752  2.79608752  2.79608752  2.0578687\n",
            "  0.59415897 -0.3095227  -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            "  0.2759612   1.76512675  2.45243393  2.79608752  2.79608752  2.79608752\n",
            "  2.79608752  2.68153633  1.26873823 -0.28406688 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389  1.30692197  2.79608752\n",
            "  2.79608752  2.79608752  2.27424318  1.29419406  1.25601032 -0.22042732\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
            " -0.42407389 -0.42407389 -0.42407389 -0.42407389]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert class vectors to binary class matrices\n"
      ],
      "metadata": {
        "id": "bU4tXwM84b2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model\n"
      ],
      "metadata": {
        "id": "WXl-f6Sg4ddU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train\n"
      ],
      "metadata": {
        "id": "WJ2plfNc4gJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## evaluate on the test set\n",
        "## you should get acc higher than 0.97\n"
      ],
      "metadata": {
        "id": "U_lMqrYz4jC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uKh-bRKm5FXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}