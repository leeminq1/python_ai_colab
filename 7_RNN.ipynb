{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeminq1/python_ai_colab/blob/main/7_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwg3JWBQZuNt"
      },
      "source": [
        "## RNN (Recurrent Neural Network)\n",
        "### - For time series data, text, video, .."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt4zXYx70mqA"
      },
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import reuters\n",
        "from keras.datasets import imdb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce3DccXLtSvc"
      },
      "source": [
        "## 1. Reuters news Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuwEIDhAATUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e237d8e1-cb09-4189-e050-a54bda2f343f"
      },
      "source": [
        "## Load Dataset (https://keras.io/api/datasets/reuters/)\n",
        "max_features = 5000 # 자주나오는 5000개의 단어만\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.reuters.load_data(num_words=max_features)\n",
        "## padding \n",
        "from keras.preprocessing.sequence import pad_sequences #(https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)\n",
        "text_max_words = 120 # 한 news의 길이는 최대 120단어\n",
        "# 길이를 맞춰주기 위해서 120으로 자름\n",
        "X_train = pad_sequences(X_train, maxlen=text_max_words)\n",
        "X_test = pad_sequences(X_test, maxlen=text_max_words)\n",
        "## to categorical\n",
        "# 뒤에 46 안써도 줘도 자기가 알아서 y label 종류보고 맞춰줌\n",
        "y_train = keras.utils.to_categorical(y_train,46)\n",
        "y_test = keras.utils.to_categorical(y_test,46)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "2121728/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_train[0].shape)"
      ],
      "metadata": {
        "id": "FOsy_n7xiGdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45dd1130-ce38-4aa1-8183-d1b1088b858f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8982, 120)\n",
            "(120,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## word to index\n",
        "# 단어를 다 index로 바꿔줌\n",
        "word_to_index=reuters.get_word_index()"
      ],
      "metadata": {
        "id": "rVaifzteU3N0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb0b675-7dcd-4459-8f58-0c45aade963a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n",
            "565248/550378 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETtMvfOJxHLt",
        "outputId": "a1702a70-2489-49e7-b075-3b8845c8957b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mdbl': 10996,\n",
              " 'fawc': 16260,\n",
              " 'degussa': 12089,\n",
              " 'woods': 8803,\n",
              " 'hanging': 13796,\n",
              " 'localized': 20672,\n",
              " 'sation': 20673,\n",
              " 'chanthaburi': 20675,\n",
              " 'refunding': 10997,\n",
              " 'hermann': 8804,\n",
              " 'passsengers': 20676,\n",
              " 'stipulate': 20677,\n",
              " 'heublein': 8352,\n",
              " 'screaming': 20713,\n",
              " 'tcby': 16261,\n",
              " 'four': 185,\n",
              " 'grains': 1642,\n",
              " 'broiler': 20680,\n",
              " 'wooden': 12090,\n",
              " 'wednesday': 1220,\n",
              " 'highveld': 13797,\n",
              " 'duffour': 7593,\n",
              " '0053': 20681,\n",
              " 'elections': 3914,\n",
              " '270': 2563,\n",
              " '271': 3551,\n",
              " '272': 5113,\n",
              " '273': 3552,\n",
              " '274': 3400,\n",
              " 'rudman': 7975,\n",
              " '276': 3401,\n",
              " '277': 3478,\n",
              " '278': 3632,\n",
              " '279': 4309,\n",
              " 'dormancy': 9381,\n",
              " 'errors': 7247,\n",
              " 'deferred': 3086,\n",
              " 'sptnd': 20683,\n",
              " 'cooking': 8805,\n",
              " 'stratabit': 20684,\n",
              " 'designing': 16262,\n",
              " 'metalurgicos': 20685,\n",
              " 'databank': 13798,\n",
              " '300er': 20686,\n",
              " 'shocks': 20687,\n",
              " 'nawg': 7972,\n",
              " 'tnta': 20688,\n",
              " 'perforations': 20689,\n",
              " 'affiliates': 2891,\n",
              " '27p': 20690,\n",
              " 'ching': 16263,\n",
              " 'china': 595,\n",
              " 'wagyu': 16264,\n",
              " 'affiliated': 3189,\n",
              " 'chino': 16265,\n",
              " 'chinh': 16266,\n",
              " 'slickline': 20692,\n",
              " 'doldrums': 13799,\n",
              " 'kids': 12092,\n",
              " 'climbed': 3028,\n",
              " 'controversy': 6693,\n",
              " 'kidd': 20693,\n",
              " 'spotty': 12093,\n",
              " 'rebel': 12639,\n",
              " 'millimetres': 9382,\n",
              " 'golden': 4007,\n",
              " 'projection': 5689,\n",
              " 'stern': 12094,\n",
              " \"hudson's\": 7903,\n",
              " 'dna': 10066,\n",
              " 'dnc': 20695,\n",
              " 'hodler': 20696,\n",
              " 'lme': 2394,\n",
              " 'insolvancy': 20697,\n",
              " 'music': 13800,\n",
              " 'therefore': 1984,\n",
              " 'dns': 10998,\n",
              " 'distortions': 6959,\n",
              " 'thassos': 13801,\n",
              " 'populations': 20698,\n",
              " 'meteorologist': 8806,\n",
              " 'loss': 43,\n",
              " 'exco': 9383,\n",
              " 'adventist': 20813,\n",
              " 'murchison': 16267,\n",
              " 'locked': 10999,\n",
              " 'kampala': 13802,\n",
              " 'arndt': 20699,\n",
              " 'nakasone': 1267,\n",
              " 'steinweg': 20700,\n",
              " \"india's\": 3633,\n",
              " 'wang': 3029,\n",
              " 'wane': 10067,\n",
              " 'unjust': 13803,\n",
              " 'titanium': 13804,\n",
              " 'want': 850,\n",
              " 'pinto': 20701,\n",
              " \"institutes'\": 16268,\n",
              " 'absolute': 7973,\n",
              " 'travel': 4677,\n",
              " 'cutback': 6422,\n",
              " 'nazmi': 16269,\n",
              " 'modest': 1858,\n",
              " 'shopwell': 16270,\n",
              " 'sedi': 20702,\n",
              " 'adoped': 20703,\n",
              " 'tulis': 16271,\n",
              " '18th': 20704,\n",
              " \"wmc's\": 20705,\n",
              " 'menlo': 20706,\n",
              " 'reiners': 11000,\n",
              " 'farmlands': 12095,\n",
              " 'nonsensical': 20707,\n",
              " 'elisra': 20708,\n",
              " 'welcomed': 2461,\n",
              " 'peup': 20709,\n",
              " \"holiday's\": 16272,\n",
              " 'activating': 20711,\n",
              " 'avondale': 16273,\n",
              " 'interational': 16274,\n",
              " 'welcomes': 20712,\n",
              " 'fip': 16275,\n",
              " 'tailings': 11001,\n",
              " 'fit': 4205,\n",
              " 'lifeline': 16276,\n",
              " 'bringing': 1916,\n",
              " 'fix': 4819,\n",
              " '624': 6164,\n",
              " 'naturalite': 12096,\n",
              " 'wales': 6165,\n",
              " 'fin': 8807,\n",
              " 'fio': 11129,\n",
              " 'ceremenony': 20714,\n",
              " 'sovr': 20715,\n",
              " \"yeo's\": 20716,\n",
              " 'effects': 1788,\n",
              " 'sixteen': 13805,\n",
              " 'undeveloped': 8808,\n",
              " 'glutted': 13806,\n",
              " 'barton': 20717,\n",
              " 'froday': 20718,\n",
              " 'arrow': 10089,\n",
              " 'stabilises': 11002,\n",
              " 'allan': 6960,\n",
              " '374p': 20719,\n",
              " '393': 3891,\n",
              " '392': 4008,\n",
              " '391': 4206,\n",
              " '390': 3079,\n",
              " '397': 4550,\n",
              " '396': 6166,\n",
              " '395': 6423,\n",
              " '394': 4207,\n",
              " '399': 6961,\n",
              " '398': 4208,\n",
              " 'stabilised': 7595,\n",
              " 'smelters': 5114,\n",
              " 'oprah': 20720,\n",
              " 'orginially': 20721,\n",
              " \"tvx's\": 20722,\n",
              " 'ponomarev': 16278,\n",
              " 'enviroment': 20723,\n",
              " \"reeves'\": 20724,\n",
              " 'mason': 8363,\n",
              " 'encourage': 1670,\n",
              " 'adapt': 7596,\n",
              " 'abbott': 12776,\n",
              " 'stamping': 13808,\n",
              " 'colquiri': 20726,\n",
              " 'ambrit': 11003,\n",
              " 'strata': 8353,\n",
              " 'corrects': 4821,\n",
              " 'sandra': 11922,\n",
              " 'estimate': 859,\n",
              " 'universally': 20727,\n",
              " 'chlorine': 20728,\n",
              " 'competes': 16279,\n",
              " 'leiner': 10068,\n",
              " 'ministries': 8809,\n",
              " 'disturbed': 8810,\n",
              " 'competed': 13809,\n",
              " 'juergen': 8811,\n",
              " 'kfw': 13810,\n",
              " 'turben': 11004,\n",
              " 'reintroduced': 9384,\n",
              " 'maladies': 20729,\n",
              " 'chevron': 4101,\n",
              " 'lazere': 16280,\n",
              " 'antilles': 8812,\n",
              " 'dti': 11907,\n",
              " 'specially': 9070,\n",
              " 'bilzerian': 4678,\n",
              " 'bakelite': 13811,\n",
              " 'renovated': 20730,\n",
              " 'service': 568,\n",
              " 'payless': 16281,\n",
              " 'spiegler': 20731,\n",
              " 'needed': 831,\n",
              " 'wigglesworth': 16282,\n",
              " 'master': 6962,\n",
              " 'antonson': 13812,\n",
              " 'genesis': 20732,\n",
              " 'vismara': 13813,\n",
              " 'organically': 20734,\n",
              " \"accords'\": 20735,\n",
              " 'task': 5940,\n",
              " 'positively': 7974,\n",
              " 'feasibility': 3479,\n",
              " 'ahmed': 6963,\n",
              " \"suralco's\": 13814,\n",
              " 'awacs': 20736,\n",
              " 'idly': 16283,\n",
              " 'regulator': 20737,\n",
              " 'pseudorabies': 12097,\n",
              " 'staubli': 16284,\n",
              " 'nzi': 8813,\n",
              " 'feeling': 5115,\n",
              " '275': 3127,\n",
              " '6819': 20738,\n",
              " 'gorman': 16285,\n",
              " 'sustaining': 8354,\n",
              " 'spectrum': 9385,\n",
              " 'consenting': 20739,\n",
              " 'recapitalized': 12098,\n",
              " 'sailed': 11562,\n",
              " 'dozen': 7597,\n",
              " 'affairs': 1985,\n",
              " 'courier': 2253,\n",
              " 'kremlin': 8355,\n",
              " 'shipments': 895,\n",
              " \"aquino's\": 16286,\n",
              " 'committing': 10070,\n",
              " 'sugarcane': 5293,\n",
              " 'diminishing': 9386,\n",
              " 'vexing': 16287,\n",
              " 'simplify': 11005,\n",
              " 'mouth': 6167,\n",
              " 'steinhardt': 7248,\n",
              " 'conceded': 8814,\n",
              " 'bradford': 9387,\n",
              " 'singer': 7976,\n",
              " '5602': 20740,\n",
              " \"1987's\": 13816,\n",
              " 'tech': 4950,\n",
              " 'teck': 6424,\n",
              " 'majv': 20741,\n",
              " 'saying': 666,\n",
              " 'dickey': 16477,\n",
              " 'sweetner': 20742,\n",
              " 'teresa': 21149,\n",
              " 'ulcer': 20743,\n",
              " 'cheaply': 13817,\n",
              " 'thai': 2361,\n",
              " 'orleans': 6964,\n",
              " 'excavator': 16290,\n",
              " 'rico': 6168,\n",
              " 'lube': 12099,\n",
              " 'rick': 13818,\n",
              " 'rich': 4679,\n",
              " 'kerna': 13819,\n",
              " 'rice': 950,\n",
              " 'rica': 4209,\n",
              " 'plate': 5503,\n",
              " 'platt': 16291,\n",
              " 'altogether': 8356,\n",
              " 'jaguar': 8815,\n",
              " 'dynair': 20744,\n",
              " 'patch': 8816,\n",
              " 'ldp': 2892,\n",
              " 'boarded': 13820,\n",
              " 'precluding': 16292,\n",
              " 'clarified': 11006,\n",
              " 'sensitivity': 16293,\n",
              " 'alternative': 1511,\n",
              " 'clarifies': 11007,\n",
              " 'lots': 5116,\n",
              " 'irs': 7598,\n",
              " 'irv': 20745,\n",
              " 'iri': 13821,\n",
              " 'ira': 13822,\n",
              " 'timber': 5690,\n",
              " 'ire': 20746,\n",
              " 'discipline': 5219,\n",
              " 'extend': 1937,\n",
              " 'nature': 3634,\n",
              " \"amb's\": 16295,\n",
              " 'dunhill': 16296,\n",
              " 'extent': 2142,\n",
              " 'restrcitions': 20747,\n",
              " 'heating': 2396,\n",
              " \"mannesmann's\": 11008,\n",
              " 'outsanding': 20748,\n",
              " 'multimillions': 20749,\n",
              " 'sarcinelli': 13824,\n",
              " 'southeastern': 6694,\n",
              " 'eradicate': 10071,\n",
              " 'libyan': 9388,\n",
              " 'foreclosing': 20750,\n",
              " 'maclaine': 12101,\n",
              " 'fra': 20751,\n",
              " 'union': 353,\n",
              " 'frn': 11009,\n",
              " 'much': 386,\n",
              " 'fry': 12102,\n",
              " 'mothball': 20752,\n",
              " 'chlorazepate': 10072,\n",
              " 'dxns': 12103,\n",
              " 'toyko': 19981,\n",
              " 'spit': 20753,\n",
              " '007050': 16297,\n",
              " 'freehold': 16298,\n",
              " 'davy': 13825,\n",
              " 'dave': 11010,\n",
              " 'spie': 12177,\n",
              " 'aguayo': 10117,\n",
              " 'wildcat': 12104,\n",
              " 'fecs': 10069,\n",
              " 'kennan': 20754,\n",
              " 'intal': 16299,\n",
              " 'contingencies': 9389,\n",
              " 'professionally': 16551,\n",
              " 'microbiological': 16300,\n",
              " 'misconstrued': 20756,\n",
              " 'k': 409,\n",
              " 'securitiesd': 20757,\n",
              " 'deferring': 16301,\n",
              " 'kohl': 5941,\n",
              " 'conditioned': 3030,\n",
              " 'fnhb': 20758,\n",
              " \"october's\": 16302,\n",
              " 'memorial': 13954,\n",
              " 'democracies': 6965,\n",
              " 'conformed': 27520,\n",
              " 'split': 464,\n",
              " \"bond's\": 12105,\n",
              " 'thinly': 11112,\n",
              " 'dunkirk': 16515,\n",
              " 'cavanaugh': 16303,\n",
              " \"securities'\": 13827,\n",
              " 'marches': 21345,\n",
              " 'issam': 16304,\n",
              " 'workforce': 2020,\n",
              " 'meinert': 12106,\n",
              " 'boiler': 13828,\n",
              " \"bp's\": 5294,\n",
              " 'torpedoed': 16305,\n",
              " 'indidate': 20762,\n",
              " 'downwardly': 13829,\n",
              " 'viviez': 20763,\n",
              " 'vladiminovich': 20764,\n",
              " 'academic': 16306,\n",
              " 'architecural': 20765,\n",
              " 'corporate': 1117,\n",
              " 'appropriately': 16307,\n",
              " 'teicc': 20766,\n",
              " \"hanover's\": 20767,\n",
              " 'aristech': 8817,\n",
              " 'portrayed': 20768,\n",
              " 'raffineries': 21383,\n",
              " 'hai': 20770,\n",
              " 'hal': 7599,\n",
              " 'ham': 13830,\n",
              " 'han': 10073,\n",
              " 'e15b': 20771,\n",
              " 'had': 61,\n",
              " 'hay': 20772,\n",
              " 'botchwey': 13831,\n",
              " 'haq': 10074,\n",
              " 'has': 37,\n",
              " 'hat': 13832,\n",
              " 'hav': 20773,\n",
              " 'fortin': 20774,\n",
              " 'municipal': 8818,\n",
              " 'osman': 20775,\n",
              " 'fsical': 20776,\n",
              " 'elders': 3480,\n",
              " 'survival': 12107,\n",
              " 'unequivocally': 16308,\n",
              " 'objective': 2519,\n",
              " 'indicative': 6695,\n",
              " 'shadow': 10075,\n",
              " 'riskiness': 21411,\n",
              " 'positiive': 20778,\n",
              " \"american's\": 10076,\n",
              " 'alick': 16309,\n",
              " 'harima': 16310,\n",
              " 'alice': 12108,\n",
              " 'altschul': 20779,\n",
              " 'festivities': 16311,\n",
              " 'medecines': 20780,\n",
              " 'beneficial': 2942,\n",
              " 'yoweri': 12109,\n",
              " 'crowd': 13833,\n",
              " 'crowe': 9390,\n",
              " 'crown': 3553,\n",
              " 'topping': 13679,\n",
              " 'captive': 8819,\n",
              " 'billboard': 12110,\n",
              " 'fiduciary': 6169,\n",
              " 'bottom': 3402,\n",
              " 'plucked': 20782,\n",
              " 'locksmithing': 20783,\n",
              " 'ecopetrol': 9391,\n",
              " 'pipestone': 24018,\n",
              " \"growers'\": 5505,\n",
              " 'borrows': 20785,\n",
              " 'eduard': 16312,\n",
              " 'venpres': 13834,\n",
              " 'bamboo': 16313,\n",
              " 'foolish': 13835,\n",
              " 'uruguyan': 20786,\n",
              " 'officeholders': 20787,\n",
              " 'economiques': 20788,\n",
              " 'aden': 16314,\n",
              " 'maxwell': 4822,\n",
              " 'marshall': 4680,\n",
              " 'honeymoon': 16315,\n",
              " 'administer': 16316,\n",
              " 'shoots': 20790,\n",
              " 'rubbertech': 16317,\n",
              " 'johsen': 16318,\n",
              " 'reciprocity': 10077,\n",
              " 'fabric': 13836,\n",
              " 'suffice': 20791,\n",
              " 'spokemsan': 20792,\n",
              " \"sonora's\": 20793,\n",
              " '5865': 16319,\n",
              " \"systems'\": 16320,\n",
              " 'perfumes': 20794,\n",
              " 'halycon': 20795,\n",
              " 'nonvoting': 20796,\n",
              " 'safeguard': 7250,\n",
              " 'sawdust': 21538,\n",
              " \"else's\": 20797,\n",
              " 'arrays': 13837,\n",
              " 'aza': 20798,\n",
              " 'smasher': 20799,\n",
              " 'complications': 12111,\n",
              " 'pesos': 1813,\n",
              " 'relabelling': 20800,\n",
              " 'passenger': 3722,\n",
              " \"avon's\": 12112,\n",
              " 'megahertz': 20801,\n",
              " 'mirror': 10683,\n",
              " 'minas': 8357,\n",
              " 'bourdain': 16322,\n",
              " 'crownx': 20802,\n",
              " 'eventual': 6425,\n",
              " 'crowns': 1207,\n",
              " 'role': 1369,\n",
              " 'obliges': 20803,\n",
              " 'rolf': 16323,\n",
              " 'vegetative': 13838,\n",
              " 'rolm': 20804,\n",
              " 'roll': 4419,\n",
              " 'intend': 2463,\n",
              " 'palms': 16324,\n",
              " 'denys': 19255,\n",
              " 'transported': 13839,\n",
              " 'moresby': 20805,\n",
              " 'devon': 16325,\n",
              " 'intent': 1351,\n",
              " \"camco's\": 20806,\n",
              " 'variable': 5942,\n",
              " 'transporter': 20807,\n",
              " 'danske': 16326,\n",
              " 'friedhelm': 13840,\n",
              " 'hawker': 8358,\n",
              " \"sand's\": 17774,\n",
              " 'preseving': 20808,\n",
              " '80386': 12113,\n",
              " 'bnls': 16328,\n",
              " 'ordination': 19984,\n",
              " 'overturned': 11011,\n",
              " 'erred': 16329,\n",
              " 'cincinnati': 6696,\n",
              " 'corps': 16710,\n",
              " 'whoever': 20809,\n",
              " 'osp': 16330,\n",
              " 'osr': 13841,\n",
              " 'ost': 12114,\n",
              " 'chair': 16331,\n",
              " '690': 5647,\n",
              " 'grapples': 20810,\n",
              " 'megawatts': 13842,\n",
              " 'photocopiers': 20811,\n",
              " 'sconninx': 20812,\n",
              " 'circumstances': 2274,\n",
              " 'oversight': 13843,\n",
              " \"paradyne's\": 20814,\n",
              " '691': 6363,\n",
              " 'paychecks': 20815,\n",
              " \"stadelmann's\": 13844,\n",
              " 'choice': 3241,\n",
              " 'vastagh': 11012,\n",
              " 'embark': 8820,\n",
              " 'gloomy': 9392,\n",
              " 'stays': 9393,\n",
              " 'exact': 4009,\n",
              " 'minute': 5117,\n",
              " 'kittiwake': 11892,\n",
              " 'picul': 20816,\n",
              " 'skewed': 20817,\n",
              " 'cooke': 11013,\n",
              " 'defaults': 10078,\n",
              " 'reimpose': 11014,\n",
              " 'hindered': 9394,\n",
              " 'lengthened': 20818,\n",
              " 'chopping': 16333,\n",
              " 'mckiernan': 13845,\n",
              " 'collaspe': 20819,\n",
              " 'corazon': 7251,\n",
              " 'antwerp': 7600,\n",
              " 'abdullah': 13846,\n",
              " 'goldston': 13847,\n",
              " '300': 442,\n",
              " 'cassa': 20821,\n",
              " 'casse': 20822,\n",
              " '695': 4081,\n",
              " 'ground': 2979,\n",
              " 'boost': 839,\n",
              " 'azusa': 16334,\n",
              " 'drafted': 9395,\n",
              " '303': 4823,\n",
              " 'climbs': 13848,\n",
              " 'honour': 7601,\n",
              " 'vanderbilt': 20823,\n",
              " '305': 3968,\n",
              " 'address': 3031,\n",
              " 'dwindling': 8821,\n",
              " 'benson': 7252,\n",
              " 'enroll': 12115,\n",
              " 'revenues': 501,\n",
              " 'impacted': 12116,\n",
              " 'queue': 20826,\n",
              " 'accomplished': 10079,\n",
              " 'throughput': 7602,\n",
              " 'influx': 9396,\n",
              " 'stockbuilding': 10080,\n",
              " 'aproximates': 20827,\n",
              " 'petroleo': 13849,\n",
              " 'sistemas': 16335,\n",
              " 'feretti': 14053,\n",
              " 'opposes': 5943,\n",
              " 'working': 882,\n",
              " 'perished': 20829,\n",
              " 'oldham': 13850,\n",
              " '27000': 20830,\n",
              " 'optimize': 19245,\n",
              " 'vigour': 20832,\n",
              " 'opposed': 1580,\n",
              " 'liberalizing': 16336,\n",
              " 'wvz': 20833,\n",
              " 'dampness': 20834,\n",
              " 'approving': 13851,\n",
              " 'sierra': 13496,\n",
              " 'entrepot': 20835,\n",
              " 'currency': 224,\n",
              " 'originally': 1499,\n",
              " 'tindemans': 20837,\n",
              " 'valorem': 16337,\n",
              " 'following': 477,\n",
              " 'fossen': 20838,\n",
              " 'locke': 11016,\n",
              " 'employess': 20839,\n",
              " 'rotberg': 12117,\n",
              " 'parachute': 16338,\n",
              " 'locks': 11017,\n",
              " 'incremental': 12255,\n",
              " 'woolowrth': 16339,\n",
              " 'listens': 20841,\n",
              " 'litre': 7253,\n",
              " 'edouard': 3554,\n",
              " 'ounce': 1377,\n",
              " 'nicanor': 20843,\n",
              " 'sucocitrico': 20844,\n",
              " 'minicomputers': 16340,\n",
              " \"silva's\": 16341,\n",
              " 'restitutions': 11018,\n",
              " 'custer': 16342,\n",
              " '3rd': 2590,\n",
              " 'fueled': 10081,\n",
              " 'trydahl': 20845,\n",
              " 'aice': 11019,\n",
              " 'harmon': 12118,\n",
              " 'conscious': 10082,\n",
              " 'herbicidesand': 20846,\n",
              " 'subdivisions': 20847,\n",
              " \"veslefrikk's\": 20848,\n",
              " 'swollen': 11020,\n",
              " 'pulled': 7978,\n",
              " 'tilney': 20849,\n",
              " 'years': 203,\n",
              " 'structuring': 20850,\n",
              " 'episodes': 20851,\n",
              " 'sportscene': 16343,\n",
              " \"northair's\": 16344,\n",
              " 'jig': 20852,\n",
              " 'jin': 20853,\n",
              " 'jim': 3403,\n",
              " 'troubles': 8359,\n",
              " 'workforces': 13852,\n",
              " 'suspension': 2362,\n",
              " 'troubled': 3892,\n",
              " 'fondiaria': 16345,\n",
              " 'modestly': 6697,\n",
              " 'recipients': 12119,\n",
              " 'civilian': 7979,\n",
              " 'indigenous': 13853,\n",
              " 'overpowering': 20854,\n",
              " 'drilling': 1051,\n",
              " 'sorted': 16346,\n",
              " 'lichtenstein': 16347,\n",
              " 'bedevil': 20855,\n",
              " 'dispite': 20856,\n",
              " 'battleships': 16843,\n",
              " 'instability': 4824,\n",
              " 'quarter': 95,\n",
              " 'salado': 20857,\n",
              " 'honduras': 5692,\n",
              " \"chevron's\": 13855,\n",
              " \"lazere's\": 12273,\n",
              " 'receipt': 2660,\n",
              " 'sponsor': 8360,\n",
              " 'entering': 4825,\n",
              " \"kcbt's\": 16349,\n",
              " 'nowicki': 19987,\n",
              " 'salads': 13856,\n",
              " 'augar': 16351,\n",
              " '797': 7980,\n",
              " '796': 7254,\n",
              " '795': 8361,\n",
              " '794': 5295,\n",
              " '793': 5118,\n",
              " '792': 6170,\n",
              " '791': 5296,\n",
              " '790': 4826,\n",
              " \"nikko's\": 20858,\n",
              " 'unsaleable': 20859,\n",
              " '799': 5720,\n",
              " '798': 5693,\n",
              " 'seriously': 2143,\n",
              " 'trauma': 16352,\n",
              " 'tvbh': 20860,\n",
              " 'macedon': 20861,\n",
              " 'disintegrated': 21906,\n",
              " 'adddition': 21909,\n",
              " 'incentives': 2244,\n",
              " 'complicated': 5944,\n",
              " 'reevaluating': 20864,\n",
              " 'thatching': 21921,\n",
              " 'brasil': 7981,\n",
              " '79p': 20865,\n",
              " 'wrong': 4951,\n",
              " 'initiate': 8822,\n",
              " 'aboard': 16353,\n",
              " 'saving': 7255,\n",
              " 'spoken': 8823,\n",
              " 'parkinson': 16364,\n",
              " 'one': 65,\n",
              " 'ont': 20867,\n",
              " 'concert': 7256,\n",
              " \"boston's\": 16354,\n",
              " 'stifled': 13859,\n",
              " 'types': 4622,\n",
              " 'lingering': 20868,\n",
              " 'surges': 16356,\n",
              " 'hurdman': 20869,\n",
              " 'herds': 16357,\n",
              " 'absorbs': 14114,\n",
              " 'surged': 4681,\n",
              " 'dalkon': 14211,\n",
              " 'crossroads': 13860,\n",
              " 'shakeup': 20870,\n",
              " 'disasterous': 20871,\n",
              " 'illness': 11021,\n",
              " 'turned': 3242,\n",
              " 'locations': 3801,\n",
              " 'tyranite': 12120,\n",
              " 'minesweepers': 13861,\n",
              " 'turner': 7257,\n",
              " 'borough': 20872,\n",
              " 'underlines': 12358,\n",
              " \"bancorporation's\": 20873,\n",
              " 'fashionable': 20874,\n",
              " \"ae's\": 20875,\n",
              " 'dilutions': 16358,\n",
              " 'goodman': 9472,\n",
              " 'unlawfully': 10510,\n",
              " 'mayer': 16359,\n",
              " 'printer': 16360,\n",
              " 'offload': 20877,\n",
              " 'opposite': 13862,\n",
              " 'buffer': 738,\n",
              " 'printed': 9398,\n",
              " 'pequiven': 16361,\n",
              " 'panoche': 13863,\n",
              " 'knowingly': 20878,\n",
              " 'ecusta': 16362,\n",
              " 'thsl': 20879,\n",
              " 'phil': 8825,\n",
              " 'jitters': 13864,\n",
              " 'touche': 16363,\n",
              " 'jittery': 20881,\n",
              " 'friction': 3291,\n",
              " 'fecal': 16365,\n",
              " 'resurgance': 22068,\n",
              " 'heeding': 20882,\n",
              " 'soviets': 2363,\n",
              " 'imagined': 16366,\n",
              " 'transact': 16367,\n",
              " 'califoirnia': 20883,\n",
              " \"chrysler's\": 9399,\n",
              " 'respecitvely': 16368,\n",
              " 'presse': 16369,\n",
              " 'euromarket': 10084,\n",
              " 'guarded': 12121,\n",
              " 'satisfacotry': 16371,\n",
              " 'authroization': 20884,\n",
              " 'simplistic': 20885,\n",
              " 'monde': 20886,\n",
              " 'awaiting': 4102,\n",
              " 'recombinant': 13865,\n",
              " 'refinancement': 20887,\n",
              " 'comserv': 20888,\n",
              " 'kitakyushu': 20889,\n",
              " 'pima': 16372,\n",
              " 'basle': 11022,\n",
              " '6250': 20891,\n",
              " 'choudhury': 16373,\n",
              " 'vision': 8826,\n",
              " 'interruptible': 20892,\n",
              " 'weatherford': 13866,\n",
              " '832': 7982,\n",
              " '833': 5694,\n",
              " '830': 4420,\n",
              " '831': 5119,\n",
              " '836': 5297,\n",
              " '837': 4553,\n",
              " '834': 6172,\n",
              " '835': 4952,\n",
              " 'alarming': 22144,\n",
              " '838': 5695,\n",
              " '839': 6173,\n",
              " '524p': 20893,\n",
              " 'sponsorship': 20894,\n",
              " 'vendex': 12122,\n",
              " \"amsouth's\": 20895,\n",
              " 'kilometer': 20896,\n",
              " 'enjoys': 10086,\n",
              " 'illiberal': 20897,\n",
              " 'punta': 6174,\n",
              " 'punte': 20898,\n",
              " 'girozentrale': 10087,\n",
              " 'missstatements': 20899,\n",
              " 'marietta': 10088,\n",
              " 'awards': 6175,\n",
              " 'concentrated': 3635,\n",
              " '83p': 20900,\n",
              " 'developpement': 13867,\n",
              " 'rhodes': 13868,\n",
              " 'matheson': 5696,\n",
              " '1720': 20901,\n",
              " 'paring': 20902,\n",
              " 's': 35,\n",
              " 'concentrates': 4953,\n",
              " \"can's\": 16374,\n",
              " 'polysaturated': 22183,\n",
              " 'parini': 20903,\n",
              " 'baden': 13869,\n",
              " 'bader': 20904,\n",
              " 'buoyancy': 12123,\n",
              " 'erdem': 20905,\n",
              " 'properites': 16375,\n",
              " 'comparitive': 20906,\n",
              " 'practises': 12124,\n",
              " 'collides': 20907,\n",
              " 'west': 189,\n",
              " 'wess': 20908,\n",
              " 'collided': 13870,\n",
              " 'practised': 20909,\n",
              " \"amalgamated's\": 20910,\n",
              " 'motives': 20911,\n",
              " 'wants': 1378,\n",
              " 'formed': 1273,\n",
              " 'readings': 20912,\n",
              " 'geothermal': 12125,\n",
              " 'tightened': 7315,\n",
              " \"d'or\": 11023,\n",
              " 'former': 1109,\n",
              " 'venezulean': 20913,\n",
              " 'curd': 19935,\n",
              " 'squeezes': 12126,\n",
              " 'newspaper': 1019,\n",
              " 'situation': 817,\n",
              " 'ivey': 13871,\n",
              " 'engaged': 3636,\n",
              " 'dubious': 13872,\n",
              " 'cayacq': 17061,\n",
              " 'cobol': 20916,\n",
              " 'limping': 20917,\n",
              " 'technology': 883,\n",
              " 'koerner': 20919,\n",
              " 'debilitating': 16376,\n",
              " 'verified': 7983,\n",
              " 'otto': 4010,\n",
              " '7770': 20920,\n",
              " 'emulsions': 16377,\n",
              " \"onic's\": 16378,\n",
              " 'slate': 9075,\n",
              " 'wires': 20921,\n",
              " 'edged': 5506,\n",
              " 'assigns': 20922,\n",
              " 'singapore': 1341,\n",
              " 'deflate': 20923,\n",
              " \"strategy's\": 20924,\n",
              " 'walesa': 16379,\n",
              " 'advertisement': 4554,\n",
              " 'luyten': 20925,\n",
              " 'shrortly': 20926,\n",
              " 'corpoartion': 20927,\n",
              " 'preferance': 22290,\n",
              " 'tracking': 16380,\n",
              " 'sunnyvale': 13874,\n",
              " 'colorants': 20928,\n",
              " 'persistently': 16381,\n",
              " \"officers'\": 16382,\n",
              " \"his's\": 20929,\n",
              " 'being': 367,\n",
              " 'divestitures': 7259,\n",
              " 'steamer': 20930,\n",
              " 'rover': 20931,\n",
              " 'grounded': 8362,\n",
              " \"businessmen's\": 16383,\n",
              " 'cyanidation': 16384,\n",
              " 'overthrow': 20932,\n",
              " 'partnerhip': 20933,\n",
              " 'sumt': 16385,\n",
              " 'sums': 8827,\n",
              " 'oelmuehle': 16386,\n",
              " 'unveil': 16387,\n",
              " 'gestures': 13875,\n",
              " 'penta': 20934,\n",
              " 'traffic': 2544,\n",
              " 'preference': 2428,\n",
              " 'sumi': 20935,\n",
              " 'world': 166,\n",
              " 'postal': 9400,\n",
              " 'bced': 16388,\n",
              " 'dornbush': 12128,\n",
              " 'confine': 14215,\n",
              " '2555': 20936,\n",
              " \"zambia's\": 5945,\n",
              " 'superiority': 20937,\n",
              " 'militate': 20938,\n",
              " 'satisfactory': 2395,\n",
              " 'superintendent': 20939,\n",
              " 'tvx': 5946,\n",
              " 'tvt': 16389,\n",
              " 'magma': 6698,\n",
              " 'diving': 20940,\n",
              " 'tvb': 15548,\n",
              " 'seaman': 13876,\n",
              " 'matsunaga': 11025,\n",
              " '919': 4827,\n",
              " '918': 5298,\n",
              " 'refundable': 17070,\n",
              " '914': 5947,\n",
              " '917': 7260,\n",
              " '916': 6699,\n",
              " '911': 5507,\n",
              " '910': 4828,\n",
              " 'restoring': 10213,\n",
              " '912': 4555,\n",
              " 'squabble': 20942,\n",
              " 'retains': 7261,\n",
              " \"partner's\": 20943,\n",
              " 'leadership': 5300,\n",
              " 'graaf': 11026,\n",
              " 'spacelab': 20944,\n",
              " 'thailand': 1800,\n",
              " 'graan': 9402,\n",
              " 'exasperating': 20945,\n",
              " 'hartmarx': 12129,\n",
              " 'frights': 16390,\n",
              " 'niall': 20946,\n",
              " 'johnston': 11027,\n",
              " '91p': 16391,\n",
              " 'sensitively': 16392,\n",
              " 'porsche': 6016,\n",
              " 'prepares': 15494,\n",
              " 'lively': 12130,\n",
              " 'stoppages': 10686,\n",
              " \"associated's\": 16394,\n",
              " 'pivot': 12131,\n",
              " 'series': 1037,\n",
              " 'sese': 24050,\n",
              " 'bubble': 7604,\n",
              " 'trusses': 16395,\n",
              " 'interestate': 20949,\n",
              " 'continents': 20950,\n",
              " 'societal': 20951,\n",
              " 'with': 28,\n",
              " 'pull': 6176,\n",
              " 'rush': 6700,\n",
              " 'monopoly': 6222,\n",
              " 'operationally': 20953,\n",
              " 'dirty': 20954,\n",
              " 'abuses': 10090,\n",
              " 'prudhoe': 7262,\n",
              " 'pulp': 5949,\n",
              " 'rust': 16396,\n",
              " 'hellman': 20955,\n",
              " 'amdec': 20956,\n",
              " 'australasian': 16397,\n",
              " 'watches': 13878,\n",
              " 'hypertension': 20957,\n",
              " \"hemdale's\": 20958,\n",
              " 'formulation': 16398,\n",
              " 'watched': 7605,\n",
              " 'jargon': 20959,\n",
              " 'cream': 13879,\n",
              " 'ideally': 9404,\n",
              " 'ryavec': 11028,\n",
              " 'microoganisms': 20960,\n",
              " 'indemnify': 13880,\n",
              " 'wincenty': 20961,\n",
              " 'waving': 20962,\n",
              " \"multifood's\": 20963,\n",
              " 'midges': 20964,\n",
              " 'natalie': 11029,\n",
              " 'crosbie': 13881,\n",
              " 'posible': 20965,\n",
              " 'omnibus': 13882,\n",
              " 'assetsof': 20966,\n",
              " 'tricks': 13883,\n",
              " 'rs': 16399,\n",
              " 'kilogram': 20967,\n",
              " 'pruning': 25363,\n",
              " 'dyer': 13884,\n",
              " 'dyes': 20968,\n",
              " 'legislatures': 20969,\n",
              " 'scm': 16400,\n",
              " 'sci': 9405,\n",
              " 'riedel': 20970,\n",
              " 'ceramic': 16401,\n",
              " 'unitholders': 6701,\n",
              " 'scb': 13885,\n",
              " 'dn11': 20971,\n",
              " 'conditionality': 20972,\n",
              " \"stock's\": 13807,\n",
              " 'masland': 20973,\n",
              " 'causes': 7606,\n",
              " 'riots': 10091,\n",
              " 'norf': 20974,\n",
              " 'nord': 9406,\n",
              " 'midwest': 3893,\n",
              " 'tamils': 13886,\n",
              " 'ofthe': 16402,\n",
              " \"colombia's\": 3421,\n",
              " '24th': 11030,\n",
              " 'sant': 20975,\n",
              " 'moines': 10092,\n",
              " 'electrotechnical': 22577,\n",
              " 'proceeded': 24534,\n",
              " 'sanz': 20976,\n",
              " 'insufficiently': 13887,\n",
              " 'sang': 20977,\n",
              " 'sand': 5950,\n",
              " 'bracho': 16404,\n",
              " 'small': 805,\n",
              " 'workloads': 20978,\n",
              " 'sank': 6702,\n",
              " 'kemper': 20979,\n",
              " 'abbreviated': 16405,\n",
              " 'quicker': 13888,\n",
              " '199': 3802,\n",
              " '198': 3243,\n",
              " '195': 2661,\n",
              " '194': 3080,\n",
              " '197': 4310,\n",
              " '196': 3894,\n",
              " '191': 2850,\n",
              " '190': 2199,\n",
              " '193': 3481,\n",
              " '192': 3350,\n",
              " 'past': 582,\n",
              " 'fractionation': 20980,\n",
              " 'displays': 20981,\n",
              " 'pass': 3081,\n",
              " 'investment': 202,\n",
              " 'quals': 27062,\n",
              " 'quicken': 16406,\n",
              " \"centronic's\": 20983,\n",
              " 'menswear': 20984,\n",
              " 'clock': 16407,\n",
              " 'teape': 20985,\n",
              " 'teapa': 20986,\n",
              " 'prevailed': 10093,\n",
              " 'hebei': 9407,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S116oIjN2Dh7",
        "outputId": "de322fc5-e3d9-4113-ab4a-6097f06f92c9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30979"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['the']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1VZSrP93Qny",
        "outputId": "ba12bc81-c358-4c07-f941-2c70a60948c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPX4mdzviF0B"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_max_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLGew1yF1tp9",
        "outputId": "e27995f6-105b-41c7-ed5c-e78271bef233"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czI3R62mNukW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da70f9e-bd36-4cc0-b8b5-f1dbedf3af32"
      },
      "source": [
        "## Model (https://keras.io/api/layers/)\n",
        "# input은 전체 feature고 / out_dim= 위에서 120개의 단어 하나 하나가 embedding layer를 지나면서\n",
        "# 각 단어가 128개의 vector 로 변한다.\n",
        "# output_dim은 임의로 정한다. ( word_embedding의 vector의 크기)\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "    #  masking은 0으로 된 값을 사용하지 않는 다는 것을 의미함.\n",
        "      keras.layers.Masking(mask_value=0,input_dim=text_max_words),\n",
        "      keras.layers.Embedding(input_dim=max_features,output_dim=128,input_length=text_max_words,),\n",
        "      keras.layers.LSTM(100, activation='tanh', recurrent_activation='sigmoid'),\n",
        "    \n",
        "      keras.layers.Dense(46,activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_5 (Masking)         (None, 120)               0         \n",
            "                                                                 \n",
            " embedding_7 (Embedding)     (None, 120, 128)          640000    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               91600     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 46)                4646      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 736,246\n",
            "Trainable params: 736,246\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILJCvuySSWJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275b7299-c4c8-47bb-a45a-9449701bbafd"
      },
      "source": [
        "## Train\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "# 러닝 rate / loss / optimaizer 머 쓸건지 정함\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) # keras.losses.MeanSq\n",
        "## fit\n",
        "hist = model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "127/127 [==============================] - 11s 33ms/step - loss: 2.4212 - accuracy: 0.4067 - val_loss: 2.2105 - val_accuracy: 0.4394\n",
            "Epoch 2/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 1.9522 - accuracy: 0.5020 - val_loss: 1.8727 - val_accuracy: 0.4861\n",
            "Epoch 3/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 1.9999 - accuracy: 0.4934 - val_loss: 1.9917 - val_accuracy: 0.5017\n",
            "Epoch 4/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 1.7658 - accuracy: 0.5479 - val_loss: 1.8905 - val_accuracy: 0.5539\n",
            "Epoch 5/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 1.6878 - accuracy: 0.5774 - val_loss: 1.7287 - val_accuracy: 0.5773\n",
            "Epoch 6/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 1.5513 - accuracy: 0.6086 - val_loss: 1.6173 - val_accuracy: 0.6140\n",
            "Epoch 7/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 1.4070 - accuracy: 0.6465 - val_loss: 1.5806 - val_accuracy: 0.6062\n",
            "Epoch 8/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 1.2724 - accuracy: 0.6795 - val_loss: 1.4520 - val_accuracy: 0.6552\n",
            "Epoch 9/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 1.1265 - accuracy: 0.7114 - val_loss: 1.4147 - val_accuracy: 0.6496\n",
            "Epoch 10/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 1.0239 - accuracy: 0.7375 - val_loss: 1.3912 - val_accuracy: 0.6474\n",
            "Epoch 11/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.9139 - accuracy: 0.7662 - val_loss: 1.3716 - val_accuracy: 0.6630\n",
            "Epoch 12/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.8060 - accuracy: 0.7948 - val_loss: 1.4206 - val_accuracy: 0.6585\n",
            "Epoch 13/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.7396 - accuracy: 0.8075 - val_loss: 1.3973 - val_accuracy: 0.6696\n",
            "Epoch 14/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.6397 - accuracy: 0.8336 - val_loss: 1.4383 - val_accuracy: 0.6663\n",
            "Epoch 15/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.5824 - accuracy: 0.8510 - val_loss: 1.4591 - val_accuracy: 0.6741\n",
            "Epoch 16/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.4949 - accuracy: 0.8775 - val_loss: 1.5070 - val_accuracy: 0.6774\n",
            "Epoch 17/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.4334 - accuracy: 0.8956 - val_loss: 1.5975 - val_accuracy: 0.6763\n",
            "Epoch 18/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.3910 - accuracy: 0.9055 - val_loss: 1.5723 - val_accuracy: 0.6796\n",
            "Epoch 19/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.3541 - accuracy: 0.9135 - val_loss: 1.6014 - val_accuracy: 0.6930\n",
            "Epoch 20/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.3016 - accuracy: 0.9277 - val_loss: 1.6218 - val_accuracy: 0.6763\n",
            "Epoch 21/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.2686 - accuracy: 0.9346 - val_loss: 1.6615 - val_accuracy: 0.6908\n",
            "Epoch 22/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.2340 - accuracy: 0.9453 - val_loss: 1.7492 - val_accuracy: 0.6897\n",
            "Epoch 23/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.2174 - accuracy: 0.9438 - val_loss: 1.7283 - val_accuracy: 0.6819\n",
            "Epoch 24/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.1972 - accuracy: 0.9489 - val_loss: 1.7940 - val_accuracy: 0.6719\n",
            "Epoch 25/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.1849 - accuracy: 0.9499 - val_loss: 1.7909 - val_accuracy: 0.6796\n",
            "Epoch 26/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.1698 - accuracy: 0.9516 - val_loss: 1.8572 - val_accuracy: 0.6685\n",
            "Epoch 27/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1608 - accuracy: 0.9542 - val_loss: 1.7999 - val_accuracy: 0.6808\n",
            "Epoch 28/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.1493 - accuracy: 0.9546 - val_loss: 1.8208 - val_accuracy: 0.6897\n",
            "Epoch 29/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.1354 - accuracy: 0.9569 - val_loss: 1.8698 - val_accuracy: 0.6763\n",
            "Epoch 30/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1291 - accuracy: 0.9548 - val_loss: 1.9114 - val_accuracy: 0.6707\n",
            "Epoch 31/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1221 - accuracy: 0.9566 - val_loss: 1.9222 - val_accuracy: 0.6808\n",
            "Epoch 32/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.1233 - accuracy: 0.9571 - val_loss: 1.9854 - val_accuracy: 0.6730\n",
            "Epoch 33/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.1176 - accuracy: 0.9579 - val_loss: 1.9527 - val_accuracy: 0.6707\n",
            "Epoch 34/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1112 - accuracy: 0.9576 - val_loss: 1.9661 - val_accuracy: 0.6974\n",
            "Epoch 35/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1282 - accuracy: 0.9534 - val_loss: 1.9825 - val_accuracy: 0.6874\n",
            "Epoch 36/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1048 - accuracy: 0.9569 - val_loss: 1.9910 - val_accuracy: 0.6785\n",
            "Epoch 37/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0963 - accuracy: 0.9589 - val_loss: 2.0274 - val_accuracy: 0.6841\n",
            "Epoch 38/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0893 - accuracy: 0.9597 - val_loss: 2.1047 - val_accuracy: 0.6719\n",
            "Epoch 39/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0895 - accuracy: 0.9592 - val_loss: 2.1027 - val_accuracy: 0.6774\n",
            "Epoch 40/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0884 - accuracy: 0.9599 - val_loss: 2.1207 - val_accuracy: 0.6574\n",
            "Epoch 41/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0843 - accuracy: 0.9603 - val_loss: 2.1455 - val_accuracy: 0.6719\n",
            "Epoch 42/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.0789 - accuracy: 0.9600 - val_loss: 2.1409 - val_accuracy: 0.6696\n",
            "Epoch 43/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0799 - accuracy: 0.9598 - val_loss: 2.1833 - val_accuracy: 0.6774\n",
            "Epoch 44/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.0775 - accuracy: 0.9607 - val_loss: 2.1755 - val_accuracy: 0.6763\n",
            "Epoch 45/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.0785 - accuracy: 0.9603 - val_loss: 2.2071 - val_accuracy: 0.6752\n",
            "Epoch 46/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.0829 - accuracy: 0.9586 - val_loss: 2.1716 - val_accuracy: 0.6808\n",
            "Epoch 47/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0988 - accuracy: 0.9555 - val_loss: 2.1136 - val_accuracy: 0.6774\n",
            "Epoch 48/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1065 - accuracy: 0.9534 - val_loss: 2.1460 - val_accuracy: 0.6719\n",
            "Epoch 49/50\n",
            "127/127 [==============================] - 3s 24ms/step - loss: 0.0813 - accuracy: 0.9612 - val_loss: 2.1685 - val_accuracy: 0.6796\n",
            "Epoch 50/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0772 - accuracy: 0.9615 - val_loss: 2.2294 - val_accuracy: 0.6630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lRfzpLOV8mi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3689967a-518d-401b-d1a9-51989df16b0b"
      },
      "source": [
        "## Evaluate model on the test set\n",
        "model.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 1s 9ms/step - loss: 2.3411 - accuracy: 0.6656\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.341099262237549, 0.6656277775764465]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTZSUrhCh7fp"
      },
      "source": [
        "### Stacked LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DMt28TtXsWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751d7378-2332-4349-ca22-b0b5f79563e3"
      },
      "source": [
        "## We can stack up LSTM\n",
        "## Model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "      keras.layers.Masking(mask_value=0,input_dim=text_max_words),\n",
        "      keras.layers.Embedding(input_dim=max_features,output_dim=128,input_length=text_max_words,),\n",
        "    #  layer가 여러개일 경우에는 방향이 2가지로 가서 (?) return_sequnces가 필요함.\n",
        "    #  맨마지막 LSTM의 경우는 필요없음\n",
        "      keras.layers.LSTM(100, activation='tanh', recurrent_activation='sigmoid',return_sequences=True),\n",
        "      keras.layers.LSTM(100, activation='tanh', recurrent_activation='sigmoid'),\n",
        "    \n",
        "      keras.layers.Dense(46,activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_6 (Masking)         (None, 120)               0         \n",
            "                                                                 \n",
            " embedding_8 (Embedding)     (None, 120, 128)          640000    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 120, 100)          91600     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 46)                4646      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 816,646\n",
            "Trainable params: 816,646\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7zls4GXeR_6"
      },
      "source": [
        "## Train\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "# 러닝 rate / loss / optimaizer 머 쓸건지 정함\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) # keras.losses.MeanSq\n",
        "## fit\n",
        "hist = model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate model on the test set\n",
        "model.evaluate(X_test, y_test, verbose=1)"
      ],
      "metadata": {
        "id": "N7tA2WBq2ms-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU"
      ],
      "metadata": {
        "id": "LMkLKSxL2olD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw2I63DfeXnb"
      },
      "source": [
        "## Evaluate model on the test set\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "      keras.layers.Masking(mask_value=0,input_dim=text_max_words),\n",
        "      keras.layers.Embedding(input_dim=max_features,output_dim=128,input_length=text_max_words,),\n",
        "\n",
        "      keras.layers.GRU(100),\n",
        "\n",
        "  \n",
        "      keras.layers.Dense(46,activation='softmax')\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "# 러닝 rate / loss / optimaizer 머 쓸건지 정함\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) # keras.losses.MeanSq\n",
        "## fit\n",
        "hist = model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "dB1jYKgn2tln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dfce76-8b7f-4181-900d-070262f1e930"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "127/127 [==============================] - 6s 35ms/step - loss: 2.3564 - accuracy: 0.3893 - val_loss: 1.9588 - val_accuracy: 0.4561\n",
            "Epoch 2/50\n",
            "127/127 [==============================] - 4s 29ms/step - loss: 1.7574 - accuracy: 0.5253 - val_loss: 1.7693 - val_accuracy: 0.5495\n",
            "Epoch 3/50\n",
            "127/127 [==============================] - 4s 28ms/step - loss: 1.5749 - accuracy: 0.5925 - val_loss: 1.7112 - val_accuracy: 0.5695\n",
            "Epoch 4/50\n",
            "127/127 [==============================] - 4s 29ms/step - loss: 1.4377 - accuracy: 0.6312 - val_loss: 1.5972 - val_accuracy: 0.6029\n",
            "Epoch 5/50\n",
            "127/127 [==============================] - 4s 28ms/step - loss: 1.3192 - accuracy: 0.6676 - val_loss: 1.5346 - val_accuracy: 0.6385\n",
            "Epoch 6/50\n",
            "127/127 [==============================] - 4s 28ms/step - loss: 1.2029 - accuracy: 0.6943 - val_loss: 1.5172 - val_accuracy: 0.6429\n",
            "Epoch 7/50\n",
            "127/127 [==============================] - 4s 28ms/step - loss: 1.0806 - accuracy: 0.7202 - val_loss: 1.4730 - val_accuracy: 0.6496\n",
            "Epoch 8/50\n",
            "127/127 [==============================] - 3s 27ms/step - loss: 0.9464 - accuracy: 0.7651 - val_loss: 1.5852 - val_accuracy: 0.6374\n",
            "Epoch 9/50\n",
            "127/127 [==============================] - 3s 27ms/step - loss: 0.8691 - accuracy: 0.7819 - val_loss: 1.4916 - val_accuracy: 0.6529\n",
            "Epoch 10/50\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.7661 - accuracy: 0.8086 - val_loss: 1.4558 - val_accuracy: 0.6719\n",
            "Epoch 11/50\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.6718 - accuracy: 0.8296 - val_loss: 1.4599 - val_accuracy: 0.6652\n",
            "Epoch 12/50\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.5967 - accuracy: 0.8502 - val_loss: 1.4960 - val_accuracy: 0.6730\n",
            "Epoch 13/50\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.5400 - accuracy: 0.8669 - val_loss: 1.5194 - val_accuracy: 0.6819\n",
            "Epoch 14/50\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.4840 - accuracy: 0.8790 - val_loss: 1.5433 - val_accuracy: 0.6863\n",
            "Epoch 15/50\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.4465 - accuracy: 0.8916 - val_loss: 1.5634 - val_accuracy: 0.6741\n",
            "Epoch 16/50\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.4089 - accuracy: 0.8968 - val_loss: 1.6139 - val_accuracy: 0.6730\n",
            "Epoch 17/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.3723 - accuracy: 0.9081 - val_loss: 1.6465 - val_accuracy: 0.6930\n",
            "Epoch 18/50\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.3338 - accuracy: 0.9197 - val_loss: 1.6720 - val_accuracy: 0.6919\n",
            "Epoch 19/50\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.3129 - accuracy: 0.9245 - val_loss: 1.7086 - val_accuracy: 0.6674\n",
            "Epoch 20/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.2881 - accuracy: 0.9310 - val_loss: 1.7652 - val_accuracy: 0.6730\n",
            "Epoch 21/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.2688 - accuracy: 0.9354 - val_loss: 1.8265 - val_accuracy: 0.6707\n",
            "Epoch 22/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.2473 - accuracy: 0.9406 - val_loss: 1.8070 - val_accuracy: 0.6763\n",
            "Epoch 23/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.2359 - accuracy: 0.9407 - val_loss: 1.8991 - val_accuracy: 0.6585\n",
            "Epoch 24/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.2193 - accuracy: 0.9454 - val_loss: 1.8695 - val_accuracy: 0.6785\n",
            "Epoch 25/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.2048 - accuracy: 0.9475 - val_loss: 1.9605 - val_accuracy: 0.6719\n",
            "Epoch 26/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1943 - accuracy: 0.9495 - val_loss: 1.9449 - val_accuracy: 0.6652\n",
            "Epoch 27/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1782 - accuracy: 0.9513 - val_loss: 2.0045 - val_accuracy: 0.6663\n",
            "Epoch 28/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1706 - accuracy: 0.9535 - val_loss: 2.0828 - val_accuracy: 0.6596\n",
            "Epoch 29/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1602 - accuracy: 0.9527 - val_loss: 2.1036 - val_accuracy: 0.6552\n",
            "Epoch 30/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1589 - accuracy: 0.9535 - val_loss: 2.1612 - val_accuracy: 0.6452\n",
            "Epoch 31/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1574 - accuracy: 0.9516 - val_loss: 2.2100 - val_accuracy: 0.6407\n",
            "Epoch 32/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1473 - accuracy: 0.9560 - val_loss: 2.1631 - val_accuracy: 0.6618\n",
            "Epoch 33/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1357 - accuracy: 0.9576 - val_loss: 2.2065 - val_accuracy: 0.6563\n",
            "Epoch 34/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1488 - accuracy: 0.9539 - val_loss: 2.1728 - val_accuracy: 0.6585\n",
            "Epoch 35/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1432 - accuracy: 0.9534 - val_loss: 2.2154 - val_accuracy: 0.6674\n",
            "Epoch 36/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1346 - accuracy: 0.9543 - val_loss: 2.2600 - val_accuracy: 0.6652\n",
            "Epoch 37/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1234 - accuracy: 0.9567 - val_loss: 2.2900 - val_accuracy: 0.6507\n",
            "Epoch 38/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1188 - accuracy: 0.9556 - val_loss: 2.3223 - val_accuracy: 0.6574\n",
            "Epoch 39/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1106 - accuracy: 0.9582 - val_loss: 2.2498 - val_accuracy: 0.6652\n",
            "Epoch 40/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1165 - accuracy: 0.9565 - val_loss: 2.3597 - val_accuracy: 0.6607\n",
            "Epoch 41/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1183 - accuracy: 0.9567 - val_loss: 2.3813 - val_accuracy: 0.6585\n",
            "Epoch 42/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1044 - accuracy: 0.9588 - val_loss: 2.3466 - val_accuracy: 0.6574\n",
            "Epoch 43/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0980 - accuracy: 0.9583 - val_loss: 2.3918 - val_accuracy: 0.6641\n",
            "Epoch 44/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0968 - accuracy: 0.9595 - val_loss: 2.4014 - val_accuracy: 0.6574\n",
            "Epoch 45/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0928 - accuracy: 0.9593 - val_loss: 2.4288 - val_accuracy: 0.6529\n",
            "Epoch 46/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1091 - accuracy: 0.9548 - val_loss: 2.4574 - val_accuracy: 0.6374\n",
            "Epoch 47/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1024 - accuracy: 0.9574 - val_loss: 2.3961 - val_accuracy: 0.6674\n",
            "Epoch 48/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.1043 - accuracy: 0.9563 - val_loss: 2.3634 - val_accuracy: 0.6596\n",
            "Epoch 49/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0952 - accuracy: 0.9584 - val_loss: 2.4414 - val_accuracy: 0.6440\n",
            "Epoch 50/50\n",
            "127/127 [==============================] - 3s 25ms/step - loss: 0.0863 - accuracy: 0.9589 - val_loss: 2.3997 - val_accuracy: 0.6618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate model on the test set\n",
        "model.evaluate(X_test, y_test, verbose=1)"
      ],
      "metadata": {
        "id": "dwLsy3iD2uml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1dda274-a587-4c39-ac3e-fd48446eebe5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 1s 12ms/step - loss: 2.4536 - accuracy: 0.6683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.4536361694335938, 0.6682991981506348]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoH93sUyh4va"
      },
      "source": [
        "### 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIs_L0wJeYAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b2d8f3-e0c2-4433-cb48-314445675b75"
      },
      "source": [
        "## 1d CNN\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "      keras.layers.Masking(mask_value=0,input_dim=text_max_words),\n",
        "      keras.layers.Embedding(input_dim=max_features,output_dim=128,input_length=text_max_words,),\n",
        "\n",
        "      # conv 1\n",
        "      keras.layers.Conv1D(filters=32, kernel_size=(10),padding=\"same\"),\n",
        "      keras.layers.BatchNormalization(),\n",
        "      layers.Activation('relu'),\n",
        "      # conv 2\n",
        "      layers.Conv1D(filters=32, kernel_size=(10),padding=\"same\"),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Activation('relu'),\n",
        "      #  Pooling \n",
        "     layers.MaxPooling1D(pool_size=(5),padding=\"same\" ),\n",
        "\n",
        "     layers.Flatten(),\n",
        "      keras.layers.Dense(46,activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_9 (Masking)         (None, 120)               0         \n",
            "                                                                 \n",
            " embedding_11 (Embedding)    (None, 120, 128)          640000    \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 120, 32)           40992     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 120, 32)          128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 120, 32)           0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 120, 32)           10272     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 120, 32)          128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 120, 32)           0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 24, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 768)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 46)                35374     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 726,894\n",
            "Trainable params: 726,766\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBd6GHrUiBMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fac852-b398-4895-8efd-175627ea9569"
      },
      "source": [
        "## Train\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "# 러닝 rate / loss / optimaizer 머 쓸건지 정함\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) # keras.losses.MeanSq\n",
        "## fit\n",
        "hist = model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "127/127 [==============================] - 11s 22ms/step - loss: 2.0511 - accuracy: 0.4886 - val_loss: 3.4575 - val_accuracy: 0.0567\n",
            "Epoch 2/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 1.1350 - accuracy: 0.7225 - val_loss: 3.8458 - val_accuracy: 0.0501\n",
            "Epoch 3/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.4737 - accuracy: 0.9097 - val_loss: 3.4475 - val_accuracy: 0.0890\n",
            "Epoch 4/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.3165 - accuracy: 0.9488 - val_loss: 1.7577 - val_accuracy: 0.5840\n",
            "Epoch 5/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.2624 - accuracy: 0.9553 - val_loss: 1.4192 - val_accuracy: 0.6841\n",
            "Epoch 6/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.2350 - accuracy: 0.9558 - val_loss: 1.5349 - val_accuracy: 0.6752\n",
            "Epoch 7/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.2268 - accuracy: 0.9563 - val_loss: 1.5283 - val_accuracy: 0.6752\n",
            "Epoch 8/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.2184 - accuracy: 0.9567 - val_loss: 1.6238 - val_accuracy: 0.6674\n",
            "Epoch 9/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.2049 - accuracy: 0.9567 - val_loss: 1.5339 - val_accuracy: 0.6863\n",
            "Epoch 10/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1923 - accuracy: 0.9576 - val_loss: 1.5464 - val_accuracy: 0.6885\n",
            "Epoch 11/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1837 - accuracy: 0.9568 - val_loss: 1.5815 - val_accuracy: 0.6863\n",
            "Epoch 12/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.1767 - accuracy: 0.9571 - val_loss: 1.5522 - val_accuracy: 0.6908\n",
            "Epoch 13/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1713 - accuracy: 0.9574 - val_loss: 1.6875 - val_accuracy: 0.6663\n",
            "Epoch 14/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1643 - accuracy: 0.9567 - val_loss: 1.5818 - val_accuracy: 0.6674\n",
            "Epoch 15/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.1698 - accuracy: 0.9572 - val_loss: 1.7280 - val_accuracy: 0.6707\n",
            "Epoch 16/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1502 - accuracy: 0.9604 - val_loss: 1.6019 - val_accuracy: 0.6986\n",
            "Epoch 17/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1508 - accuracy: 0.9584 - val_loss: 1.6472 - val_accuracy: 0.6796\n",
            "Epoch 18/50\n",
            "127/127 [==============================] - 2s 15ms/step - loss: 0.1470 - accuracy: 0.9578 - val_loss: 1.6552 - val_accuracy: 0.6863\n",
            "Epoch 19/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1448 - accuracy: 0.9584 - val_loss: 1.6489 - val_accuracy: 0.6852\n",
            "Epoch 20/50\n",
            "127/127 [==============================] - 2s 15ms/step - loss: 0.1452 - accuracy: 0.9567 - val_loss: 1.6812 - val_accuracy: 0.6885\n",
            "Epoch 21/50\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.1339 - accuracy: 0.9574 - val_loss: 1.6797 - val_accuracy: 0.6963\n",
            "Epoch 22/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1369 - accuracy: 0.9588 - val_loss: 1.7362 - val_accuracy: 0.6952\n",
            "Epoch 23/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1230 - accuracy: 0.9603 - val_loss: 1.6737 - val_accuracy: 0.6752\n",
            "Epoch 24/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1280 - accuracy: 0.9583 - val_loss: 1.7421 - val_accuracy: 0.6752\n",
            "Epoch 25/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1198 - accuracy: 0.9592 - val_loss: 1.7975 - val_accuracy: 0.6830\n",
            "Epoch 26/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1176 - accuracy: 0.9594 - val_loss: 1.6481 - val_accuracy: 0.6941\n",
            "Epoch 27/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1146 - accuracy: 0.9573 - val_loss: 1.7534 - val_accuracy: 0.6930\n",
            "Epoch 28/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1108 - accuracy: 0.9586 - val_loss: 1.7515 - val_accuracy: 0.6808\n",
            "Epoch 29/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1080 - accuracy: 0.9590 - val_loss: 1.8339 - val_accuracy: 0.6885\n",
            "Epoch 30/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1100 - accuracy: 0.9597 - val_loss: 1.7588 - val_accuracy: 0.7030\n",
            "Epoch 31/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1088 - accuracy: 0.9587 - val_loss: 1.7133 - val_accuracy: 0.6952\n",
            "Epoch 32/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.1043 - accuracy: 0.9590 - val_loss: 1.8772 - val_accuracy: 0.6796\n",
            "Epoch 33/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.1017 - accuracy: 0.9587 - val_loss: 1.7371 - val_accuracy: 0.6986\n",
            "Epoch 34/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.1004 - accuracy: 0.9582 - val_loss: 1.9114 - val_accuracy: 0.6885\n",
            "Epoch 35/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0982 - accuracy: 0.9590 - val_loss: 1.8837 - val_accuracy: 0.6941\n",
            "Epoch 36/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0973 - accuracy: 0.9579 - val_loss: 1.8422 - val_accuracy: 0.6874\n",
            "Epoch 37/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0955 - accuracy: 0.9577 - val_loss: 1.8364 - val_accuracy: 0.6852\n",
            "Epoch 38/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0918 - accuracy: 0.9571 - val_loss: 1.9841 - val_accuracy: 0.6919\n",
            "Epoch 39/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0931 - accuracy: 0.9563 - val_loss: 1.8215 - val_accuracy: 0.6986\n",
            "Epoch 40/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0902 - accuracy: 0.9587 - val_loss: 2.0725 - val_accuracy: 0.6841\n",
            "Epoch 41/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.0906 - accuracy: 0.9578 - val_loss: 1.9825 - val_accuracy: 0.6841\n",
            "Epoch 42/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0855 - accuracy: 0.9583 - val_loss: 1.9507 - val_accuracy: 0.6863\n",
            "Epoch 43/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0856 - accuracy: 0.9571 - val_loss: 1.9136 - val_accuracy: 0.6908\n",
            "Epoch 44/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.0837 - accuracy: 0.9593 - val_loss: 2.0499 - val_accuracy: 0.6908\n",
            "Epoch 45/50\n",
            "127/127 [==============================] - 2s 13ms/step - loss: 0.0825 - accuracy: 0.9574 - val_loss: 1.9621 - val_accuracy: 0.6997\n",
            "Epoch 46/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0840 - accuracy: 0.9588 - val_loss: 1.9782 - val_accuracy: 0.6963\n",
            "Epoch 47/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0834 - accuracy: 0.9595 - val_loss: 2.0687 - val_accuracy: 0.6863\n",
            "Epoch 48/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0839 - accuracy: 0.9590 - val_loss: 2.0094 - val_accuracy: 0.6819\n",
            "Epoch 49/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0844 - accuracy: 0.9584 - val_loss: 2.0897 - val_accuracy: 0.6963\n",
            "Epoch 50/50\n",
            "127/127 [==============================] - 2s 14ms/step - loss: 0.0799 - accuracy: 0.9588 - val_loss: 2.1584 - val_accuracy: 0.6952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zOaGfL-jndZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60e36a6-ecd4-4d69-ca6d-36a4e07e3275"
      },
      "source": [
        "## Evaluate model on the test set\n",
        "model.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 1s 8ms/step - loss: 2.0748 - accuracy: 0.6932\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.0747945308685303, 0.6932324171066284]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. IMDB Movie review Classification (Kaggle Competition!)\n",
        "#### - a. load dataset: https://keras.io/api/datasets/imdb/\n",
        "#### - b. define your RNN model (Google it!) (https://keras.io/api/layers/recurrent_layers/)\n",
        "#### - c. Train your model\n",
        "#### - d. Tune hyperparamters of your model on the validation set\n",
        "#### - e. Evaluate your model on the test set.\n",
        "\n",
        "\\\n",
        "#### after you achieve your best performance, submit your result:\n",
        "#### make your team name as \"date_name\" (ex. 20220307_권원빈) ('team'탭에 가면 변경가능)\n",
        "(https://www.kaggle.com/c/imdb-classification1234/)\n",
        "\n"
      ],
      "metadata": {
        "id": "iI-T398l5EWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Dataset\n",
        "max_features = 10000 # 자주나오는 단어 10000개만\n",
        "text_max_words = 500 # 리뷰 하나는 단어 500개까지 보겠다\n",
        "(X_train, y_train), (X_test, y_test) = \n",
        "\n",
        "## padding\n",
        "X_train = \n",
        "X_test = "
      ],
      "metadata": {
        "id": "51MZ8Ffl5PUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## word to index & index_to_word\n",
        "word_to_index =  ## 사실 3칸 밀려있음. 밑에서 수정해서 쓰기!!\n"
      ],
      "metadata": {
        "id": "hiuInkvp5lzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## review sample\n"
      ],
      "metadata": {
        "id": "kYnLwMvp7tXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "      \n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "KRFUCanj70Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training\n",
        "batch_size=64\n",
        "epochs=5\n",
        "\n",
        "## compile\n",
        "\n",
        "## fit"
      ],
      "metadata": {
        "id": "jOWMNncZ-z-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate model on the test set\n",
        "model.evaluate(X_test, y_test, verbose=1)"
      ],
      "metadata": {
        "id": "NkV7PKgR-00R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Submission for Kaggle\n",
        "import pandas as pd\n",
        "\n",
        "# predict results\n",
        "results = model.predict(X_test)\n",
        "results = np.round_(results).astype(int).reshape(-1)\n",
        "results = pd.Series(results,name=\"Category\")\n",
        "\n",
        "submission = pd.concat([pd.Series(range(1,25001),name = \"Id\"),results],axis = 1)\n",
        "submission.to_csv(\"imdb_classification.csv\",index=False)"
      ],
      "metadata": {
        "id": "F10rlO0AESzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test on my sentence!!\n",
        "아래의 example을 model에 넣고 sentiment를 예측해보자"
      ],
      "metadata": {
        "id": "J6MtE5Xy4vLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## test example\n",
        "#example = 'This movie is very good. I like the characters and the story was beautiful. Also the background music was selected appropriately'\n",
        "#example = \"This movie was awful. I don't like the main characters and the story did not make any sense.\"\n",
        "example = 'i love this movie'\n",
        "\n",
        "# 1. 단어 단위로 자르시고. example.split()\n",
        "\n",
        "\n",
        "# 2. words의 각 단어를 index로 바꾸기\n",
        "#     a. 먼저, 단어를 모두 소문자로 바꾸어주어야함 & '.'은 없애야함\n",
        "#     b. index로 바꿀때, 'word_to_index[word]+3' 사용 (0,1,2,3은 token용)\n",
        "#     b. 리뷰의 맨 앞은 무조건 1로 시작 (시작 token)\n",
        "#     c. index가 10000보다 큰 단어는 2로 바꾸기 (모르는 단어용 token)\n",
        "#     d. 리뷰의 길이는 120으로 고정 (0 for padding)\n",
        "word_to_index = imdb.get_word_index()\n",
        "\n",
        "\n",
        "# 3. 길이가 500이 되도록 padding (앞부분 참고)\n",
        "\n",
        "\n",
        "# 4. model에 넣고 prediction (model.predict())\n",
        "\n"
      ],
      "metadata": {
        "id": "fROa1gv4-0qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IoI1NocUY9CX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}