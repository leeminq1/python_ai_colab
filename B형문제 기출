///////
%matplotlib inline
# Ignore the warnings
import warnings
# warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

# System related and data input controls
import os

# Data manipulation and visualization
import pandas as pd
pd.options.display.float_format = '{:,.2f}'.format
pd.options.display.max_rows = 10
pd.options.display.max_columns = 20
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Modeling algorithms
# General
import statsmodels.api as sm
from scipy import stats

# Model selection
from sklearn.model_selection import train_test_split

# Evaluation metrics
# for regression
from sklearn.metrics import mean_squared_log_error, mean_squared_error,  r2_score, mean_absolute_error
# radom forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.linear_model import LogisticRegression



///
# 평가지표 출력하는 함수 설정
def get_clf_eval(X_test,y_test, y_pred,model=None):
    confusion = confusion_matrix(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    F1 = f1_score(y_test, y_pred)
    AUC = roc_auc_score(y_test, y_pred)
    
    print('오차행렬:\n', confusion)
    print('\n정확도: {:.4f}'.format(accuracy))
    print('정밀도: {:.4f}'.format(precision))
    print('재현율: {:.4f}'.format(recall))
    print('F1: {:.4f}'.format(F1))
    print('AUC: {:.4f}'.format(AUC))
    if model:
#         print(f'feautre coefficient : {model.coef_}')
        print(f'coef_ max value : { max(model.coef_.tolist()[0]) }')
        print(f'coef_ min value : { min(model.coef_.tolist()[0]) }')
        for d in zip(list(X_test.columns), model.coef_.tolist()[0]):
            print(f"column : {d[0]} , coef_: {d[1]}") # 컬럼 이름과 계수
            

/////
# location = 'https://raw.githubusercontent.com/cheonbi/DataScience/master/Data/Bike_Sharing_Demand_Full.csv'
input_location='./input.csv'
output_loacation='./output.csv'


# header None으로 불러오기
# raw_train = pd.read_csv(input_location,header=None,prefix="X")
raw_train = pd.read_csv(input_location,header=None,prefix='X')
raw_output = pd.read_csv(output_loacation,names = ['Results'])

# shape확인
print("X_raw",raw_train.shape)
print("y_raw",raw_output.shape)


# train data column 이름을 변경
# X_train.columns = ['a', 'b',"c","d","e"]
# X_test.columns = ['a', 'b',"c","d","e"]



////
# 결측치 확인
# 결측치 확인
raw_train.isnull().sum()
raw_output.isnull().sum()



/////
# 시각화 확인
# from matplotlib import pyplot as plt
figure, axes = plt.subplots(2, 1, figsize=(80,50))
raw_train.plot(kind='line', figsize=(20,20), linewidth=3, fontsize=20,ax=axes[0]).set_title("X_train")
raw_output.plot(kind='line', figsize=(20,20), linewidth=3, fontsize=20,ax=axes[1]).set_title("y_train")



//////
raw_all = pd.concat([raw_train,raw_output],axis=1)

# # kde plot
# figure, axes = plt.subplots(5, 1, figsize=(20,40))

# sns.kdeplot(data=raw_all, x="X0", hue="Results", ax=axes[0])
# sns.kdeplot(data=raw_all, x="X1", hue="Results", ax=axes[1])
# sns.kdeplot(data=raw_all, x="X2", hue="Results", ax=axes[2])
# sns.kdeplot(data=raw_all, x="X3", hue="Results", ax=axes[3])
# sns.kdeplot(data=raw_all, x="X4", hue="Results", ax=axes[4])
feature_number=raw_train.shape[1]


#define number of rows and columns for subplots
nrow=5
ncol=4

# plot counter
figure, axes = plt.subplots(nrow, ncol)
figure.set_size_inches(20, 20)
count=0
for r in range(nrow):
    for c in range(ncol):
        sns.kdeplot(data=raw_all, x=raw_all.columns[count], hue="Results", ax=axes[r,c])
        count+=1





////////////
# train / test data 분리
X_train, X_test, y_train, y_test = train_test_split(raw_train,raw_output, test_size=0.2, shuffle=False)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)


/////////////
# 일단 한번 넣고 돌려봄
#기본적인 randomforest모형
RF_model = RandomForestClassifier(n_estimators=20, max_depth=5,random_state=0)
RF_model.fit(X_train,y_train)
predict_RF = RF_model.predict(X_test)
print("*****F.E 전 RandomForest ")
get_clf_eval(X_test,y_test,predict_RF)
print("////////////////")
# Logistic regreesion
LR_model=LogisticRegression()
LR_model.fit(X_train,y_train)
predict_LR = LR_model.predict(X_test)
print("*****F.E 전 Logistic ")
get_clf_eval(X_test,y_test,predict_LR,LR_model)




//////////////////
# 데이터 정규화 후
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#기본적인 randomforest모형
RF_model = RandomForestClassifier(n_estimators=20, max_depth=5,random_state=0)
RF_model.fit(X_train_scaled,y_train)
predict_RF = RF_model.predict(X_test_scaled)
print("*****F.E 후 RandomForest ")
get_clf_eval(X_test,y_test,predict_RF)
print("////////////////")
LR_model=LogisticRegression()
LR_model.fit(X_train,y_train)
predict_LR = LR_model.predict(X_test)
print("*****F.E 후 Logistic ")
get_clf_eval(X_test,y_test,predict_LR,LR_model)



//////////////
# random forest feature importance
ftr_importances_values = RF_model.feature_importances_
ftr_importances = pd.Series(ftr_importances_values, index = X_train.columns)
ftr_top5 = ftr_importances.sort_values(ascending=False)[:5]

plt.figure(figsize=(20,20))
plt.title('Top 5 Feature Importances')

bar=sns.barplot(x=ftr_top5, y=ftr_top5.index)
bar.set_xlabel("Impotances", fontsize=20) 
bar.set_ylabel("Feature", fontsize=20)

plt.show()
            




